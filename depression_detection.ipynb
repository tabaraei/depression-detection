{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tabaraei/depression-detection/blob/master/depression_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaSlFvp62Wul",
        "outputId": "3b234d48-ac9b-476b-8bc0-afb886d0d5cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wave\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "import LOUPE_Keras.loupe_keras as lpk"
      ],
      "metadata": {
        "id": "aMfdenYjHHJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = '/content/drive/MyDrive/Data/DepressionDetection/EATD-Corpus'\n",
        "X_train, y_train = list(), list()\n",
        "X_test, y_test = list(), list()\n",
        "\n",
        "def load_wave(data_path):\n",
        "    data_file = wave.open(data_path)\n",
        "    sr = data_file.getframerate()\n",
        "    nframes = data_file.getnframes()\n",
        "    wave_data = np.frombuffer(data_file.readframes(nframes), dtype=np.short).astype(float)\n",
        "    length = nframes / sr\n",
        "    return data_file, wave_data, length, nframes, sr\n",
        "\n",
        "def wav2mel(wave_data, sr):\n",
        "    cluster_size = 16\n",
        "    signal = wave_data\n",
        "    melspec = librosa.feature.melspectrogram(y=signal, sr=sr, n_mels=80).astype(np.float32).T\n",
        "    melspec = np.log(np.maximum(1e-6, melspec))\n",
        "    return melspec\n",
        "\n",
        "for sample in os.listdir(DATA_DIR):\n",
        "    positive_file, wave_data1, len1, nframes1, sr1 = load_wave(f'{DATA_DIR}/{sample}/positive_out.wav')\n",
        "    neutral_file, wave_data2, len2, nframes2, sr2 = load_wave(f'{DATA_DIR}/{sample}/neutral_out.wav')\n",
        "    negative_file, wave_data3, len3, nframes3, sr3 = load_wave(f'{DATA_DIR}/{sample}/negative_out.wav')\n",
        "\n",
        "    with open(f'{DATA_DIR}/{sample}/new_label.txt') as label:\n",
        "        target = float(label.readline())\n",
        "\n",
        "    if wave_data1.shape[0] < 1:\n",
        "        wave_data1 = np.array([1e-4]*sr1*5)\n",
        "    if wave_data2.shape[0] < 1:\n",
        "        wave_data2 = np.array([1e-4]*sr2*5)\n",
        "    if wave_data3.shape[0] < 1:\n",
        "        wave_data3 = np.array([1e-4]*sr3*5)\n",
        "\n",
        "    audio_features = [\n",
        "        wav2mel(wave_data1, sr1),\n",
        "        wav2mel(wave_data2, sr2),\n",
        "        wav2mel(wave_data3, sr3)\n",
        "    ]\n",
        "    # targets.append(1 if target >= 53 else 0)\n",
        "\n",
        "    if sample.startswith('t'):\n",
        "        # Training Data\n",
        "        X_train.append(audio_features)\n",
        "        y_train.append(target)\n",
        "    else:\n",
        "        # Test Data\n",
        "        X_test.append(audio_features)\n",
        "        y_test.append(target)\n",
        "    break\n",
        "\n",
        "print(X_test[0][0].shape)\n",
        "print(X_test[0][1].shape)\n",
        "print(X_test[0][2].shape)"
      ],
      "metadata": {
        "id": "HZuA1hJzCk8Q",
        "outputId": "4e03dfe5-d074-45ff-beee-68a0a9d16836",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1068, 80)\n",
            "(1201, 80)\n",
            "(930, 80)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the CNN architecture\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = self.pool(torch.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the data loaders and transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNN(num_classes=len(classes)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Testing the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
      ],
      "metadata": {
        "id": "ws92jvDbpOtB",
        "outputId": "ec7b9c9c-55dc-4583-888c-ea1355f475b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 56997282.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "[1,  2000] loss: 2.111\n",
            "[1,  4000] loss: 1.692\n",
            "[1,  6000] loss: 1.494\n",
            "[1,  8000] loss: 1.380\n",
            "[1, 10000] loss: 1.318\n",
            "[1, 12000] loss: 1.228\n",
            "[2,  2000] loss: 1.105\n",
            "[2,  4000] loss: 1.048\n",
            "[2,  6000] loss: 1.006\n",
            "[2,  8000] loss: 0.959\n",
            "[2, 10000] loss: 0.945\n",
            "[2, 12000] loss: 0.907\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 68 %\n"
          ]
        }
      ]
    }
  ]
}