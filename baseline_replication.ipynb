{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXKy2jskaRnuSjDgeQL6sk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tabaraei/depression-detection/blob/master/baseline_replication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LecNXSaQ3Erq",
        "outputId": "bbfe2e83-5ab5-4d46-8f83-fdd0f2740759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `audio_features_whole.py`"
      ],
      "metadata": {
        "id": "5ITC9B9Sx1nZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wave\n",
        "import librosa\n",
        "from python_speech_features import *\n",
        "import sys\n",
        "import pickle\n",
        "sys.path.append('/Users/linlin/Desktop/depression/classfication')\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "import vggish.vggish_input as vggish_input\n",
        "import vggish.vggish_params as vggish_params\n",
        "import vggish.vggish_postprocess as vggish_postprocess\n",
        "import vggish.vggish_slim as vggish_slim\n",
        "\n",
        "import loupe_keras as lpk\n",
        "\n",
        "from allennlp.commands.elmo import ElmoEmbedder"
      ],
      "metadata": {
        "id": "XKhG1D4myDLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.enable_eager_execution()\n",
        "\n",
        "elmo = ElmoEmbedder()\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "prefix = os.path.abspath(os.path.join(os.getcwd(), \".\"))\n",
        "\n",
        "# Paths to downloaded VGGish files.\n",
        "checkpoint_path =os.path.join(os.getcwd(),  'vggish/vggish_model.ckpt')\n",
        "pca_params_path = os.path.join(os.getcwd(), 'vggish/vggish_pca_params.npz')\n",
        "\n",
        "cluster_size = 16\n",
        "\n",
        "min_len = 100\n",
        "max_len = -1"
      ],
      "metadata": {
        "id": "PhuQ6C29yATF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_vggish_embedds(x, sr):\n",
        "    # x为输入的音频，sr为sample_rate\n",
        "    input_batch = vggish_input.waveform_to_examples(x, sr)\n",
        "    with tf.Graph().as_default(), tf.Session() as sess:\n",
        "      vggish_slim.define_vggish_slim()\n",
        "      vggish_slim.load_vggish_slim_checkpoint(sess, checkpoint_path)\n",
        "\n",
        "      features_tensor = sess.graph.get_tensor_by_name(vggish_params.INPUT_TENSOR_NAME)\n",
        "      embedding_tensor = sess.graph.get_tensor_by_name(vggish_params.OUTPUT_TENSOR_NAME)\n",
        "      [embedding_batch] = sess.run([embedding_tensor],\n",
        "                                   feed_dict={features_tensor: input_batch})\n",
        "\n",
        "    # Postprocess the results to produce whitened quantized embeddings.\n",
        "    pproc = vggish_postprocess.Postprocessor(pca_params_path)\n",
        "    postprocessed_batch = pproc.postprocess(embedding_batch)\n",
        "\n",
        "    return tf.cast(postprocessed_batch, dtype='float32')"
      ],
      "metadata": {
        "id": "Jv8hHK__yFcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wav2vlad(wave_data, sr):\n",
        "    global cluster_size\n",
        "    signal = wave_data\n",
        "    melspec = librosa.feature.melspectrogram(signal, n_mels=80,sr=sr).astype(np.float32).T\n",
        "    melspec = np.log(np.maximum(1e-6, melspec))\n",
        "    feature_size = melspec.shape[1]\n",
        "    max_samples = melspec.shape[0]\n",
        "    output_dim = cluster_size * 16\n",
        "    feat = lpk.NetVLAD(feature_size=feature_size, max_samples=max_samples, \\\n",
        "                            cluster_size=cluster_size, output_dim=output_dim) \\\n",
        "                                (tf.convert_to_tensor(melspec))\n",
        "    with tf.Session() as sess:\n",
        "        init = tf.global_variables_initializer()\n",
        "        sess.run(init)\n",
        "        r = feat.numpy()\n",
        "    return r"
      ],
      "metadata": {
        "id": "5BimqtP7yI50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(number, audio_features, targets, path):\n",
        "    global max_len, min_len\n",
        "    if not os.path.exists(os.path.join(prefix, '{1}/{0}/positive_out.wav'.format(number, path))):\n",
        "        return\n",
        "    positive_file = wave.open(os.path.join(prefix, '{1}/{0}/positive_out.wav'.format(number, path)))\n",
        "    sr1 = positive_file.getframerate()\n",
        "    nframes1 = positive_file.getnframes()\n",
        "    wave_data1 = np.frombuffer(positive_file.readframes(nframes1), dtype=np.short).astype(np.float)\n",
        "    len1 = nframes1 / sr1\n",
        "\n",
        "    neutral_file = wave.open(os.path.join(prefix, '{1}/{0}/neutral_out.wav'.format(number, path)))\n",
        "    sr2 = neutral_file.getframerate()\n",
        "    nframes2 = neutral_file.getnframes()\n",
        "    wave_data2 = np.frombuffer(neutral_file.readframes(nframes2), dtype=np.short).astype(np.float)\n",
        "    len2 = nframes2 / sr2\n",
        "\n",
        "    negative_file = wave.open(os.path.join(prefix, '{1}/{0}/negative_out.wav'.format(number, path)))\n",
        "    sr3 = negative_file.getframerate()\n",
        "    nframes3 = negative_file.getnframes()\n",
        "    wave_data3 = np.frombuffer(negative_file.readframes(nframes3), dtype=np.short).astype(np.float)\n",
        "    len3 = nframes3/sr3\n",
        "\n",
        "    for l in [len1, len2, len3]:\n",
        "        if l > max_len:\n",
        "            max_len = l\n",
        "        if l < min_len:\n",
        "            min_len = l\n",
        "\n",
        "    with open(os.path.join(prefix, '{1}/{0}/new_label.txt'.format(number, path))) as fli:\n",
        "        target = float(fli.readline())\n",
        "\n",
        "    if wave_data1.shape[0] < 1:\n",
        "        wave_data1 = np.array([1e-4]*sr1*5)\n",
        "    if wave_data2.shape[0] < 1:\n",
        "        wave_data2 = np.array([1e-4]*sr2*5)\n",
        "    if wave_data3.shape[0] < 1:\n",
        "        wave_data3 = np.array([1e-4]*sr3*5)\n",
        "    audio_features.append([wav2vlad(wave_data1, sr1), wav2vlad(wave_data2, sr2), \\\n",
        "        wav2vlad(wave_data3, sr3)])\n",
        "    # targets.append(1 if target >= 53 else 0)\n",
        "    targets.append(target)"
      ],
      "metadata": {
        "id": "Iq2MxbheyMtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_features = []\n",
        "audio_targets = []\n",
        "\n",
        "for index in range(114):\n",
        "    extract_features(index+1, audio_features, audio_targets, 'Data')\n",
        "\n",
        "for index in range(114):\n",
        "    extract_features(index+1, audio_features, audio_targets, 'ValidationData')\n",
        "\n",
        "\n",
        "print(\"Saving npz file locally...\")\n",
        "np.savez(os.path.join(prefix, 'Features/AudioWhole/whole_samples_reg_%d.npz'%(cluster_size*16)), audio_features)\n",
        "np.savez(os.path.join(prefix, 'Features/AudioWhole/whole_labels_reg_%d.npz')%(cluster_size*16), audio_targets)\n",
        "\n",
        "print(max_len, min_len)"
      ],
      "metadata": {
        "id": "m5Zs9wf33IUS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}