{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4d5m3lwOz0wKzyNuohEny",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "59a6aa0154074fba8eaa223ae334a857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0e93bc0da3e47d5909ee27c2f30505d",
              "IPY_MODEL_16ad6b8178554a4ab7dd7aaaf97799f0",
              "IPY_MODEL_76ed8d04db9b4039b55b4c0efb2da419"
            ],
            "layout": "IPY_MODEL_d38aa751fefc4c68913bb7fc32bb4c42"
          }
        },
        "c0e93bc0da3e47d5909ee27c2f30505d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56e704b9bf4440628c5daf2f36fe5ac3",
            "placeholder": "​",
            "style": "IPY_MODEL_67e01c8a850846da8eac2d0d34ad12d6",
            "value": "100%"
          }
        },
        "16ad6b8178554a4ab7dd7aaaf97799f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dc820ffde0e467e92068b3c278ce33e",
            "max": 114,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26bad87aaa9248a2a3ec7066626b262a",
            "value": 114
          }
        },
        "76ed8d04db9b4039b55b4c0efb2da419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02c5a0c6465b41c99e96d89dc7aa2b95",
            "placeholder": "​",
            "style": "IPY_MODEL_4d723aef81d34be4a98d85b3b112942b",
            "value": " 114/114 [00:00&lt;00:00, 283.02it/s]"
          }
        },
        "d38aa751fefc4c68913bb7fc32bb4c42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56e704b9bf4440628c5daf2f36fe5ac3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67e01c8a850846da8eac2d0d34ad12d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0dc820ffde0e467e92068b3c278ce33e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26bad87aaa9248a2a3ec7066626b262a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "02c5a0c6465b41c99e96d89dc7aa2b95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d723aef81d34be4a98d85b3b112942b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a38ecf4614048c68b1102fb3f7b87a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8b0f05ba2cc442e9ef0816a43938218",
              "IPY_MODEL_7f592e7f16e045c894d4867a1460af5d",
              "IPY_MODEL_f9999eaa4571419d83af11480ba03462"
            ],
            "layout": "IPY_MODEL_992574e032514d43b3d662d1ade38217"
          }
        },
        "a8b0f05ba2cc442e9ef0816a43938218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1453550fc384d048189b3e9e05868a3",
            "placeholder": "​",
            "style": "IPY_MODEL_cff82e6f04534990afb25c2c53f1f285",
            "value": "100%"
          }
        },
        "7f592e7f16e045c894d4867a1460af5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65556c90e11346739f1946885dca1865",
            "max": 114,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9691c61c0f942fa89eb2a022694aa48",
            "value": 114
          }
        },
        "f9999eaa4571419d83af11480ba03462": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3d7531b353349deae20ce864c8a3b99",
            "placeholder": "​",
            "style": "IPY_MODEL_1949b10a15924d789acddeda2e675f2b",
            "value": " 114/114 [00:00&lt;00:00, 337.60it/s]"
          }
        },
        "992574e032514d43b3d662d1ade38217": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1453550fc384d048189b3e9e05868a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cff82e6f04534990afb25c2c53f1f285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65556c90e11346739f1946885dca1865": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9691c61c0f942fa89eb2a022694aa48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3d7531b353349deae20ce864c8a3b99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1949b10a15924d789acddeda2e675f2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "444ac9c047d348a7a743e719d34d5357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69bacb88b3854f6e97ab039a3325b641",
              "IPY_MODEL_23355b514bb5445ea7cc98db021cd983",
              "IPY_MODEL_e8820441c56940cea37d2576066351e1"
            ],
            "layout": "IPY_MODEL_8b4fea35e89243b4b3ce74e16375bb74"
          }
        },
        "69bacb88b3854f6e97ab039a3325b641": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7114c0e18b35411b9996742fa0507283",
            "placeholder": "​",
            "style": "IPY_MODEL_a8cb1ce9b0ee4960a6a8df991c6e7a03",
            "value": "100%"
          }
        },
        "23355b514bb5445ea7cc98db021cd983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efc451e52ea54afda02d3a43a454afe6",
            "max": 114,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46e0c7ae81164d808276f1c012bf9b72",
            "value": 114
          }
        },
        "e8820441c56940cea37d2576066351e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_969a402499fa445ba4ee86e1b77beab5",
            "placeholder": "​",
            "style": "IPY_MODEL_1957c7190f824f6b99b6ecd06678335c",
            "value": " 114/114 [04:24&lt;00:00,  2.18s/it]"
          }
        },
        "8b4fea35e89243b4b3ce74e16375bb74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7114c0e18b35411b9996742fa0507283": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8cb1ce9b0ee4960a6a8df991c6e7a03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efc451e52ea54afda02d3a43a454afe6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46e0c7ae81164d808276f1c012bf9b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "969a402499fa445ba4ee86e1b77beab5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1957c7190f824f6b99b6ecd06678335c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9166a6768d6646659927f127514db35f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b5d19865740540edb94a1f0aa2565161",
              "IPY_MODEL_803cf4f39e02434a9c47fb9b3dc88e94",
              "IPY_MODEL_3f5c0ecc39854b1a9e59b9819a1a004f"
            ],
            "layout": "IPY_MODEL_dfcb3c129bb4474795b5ff39854676d2"
          }
        },
        "b5d19865740540edb94a1f0aa2565161": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ec9a908a012446ab8b152bba9ccb7ce",
            "placeholder": "​",
            "style": "IPY_MODEL_a1d540f250754fb2b68a7a583ddc5ac9",
            "value": "100%"
          }
        },
        "803cf4f39e02434a9c47fb9b3dc88e94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88966dffcdb746dd9f9e60a6d547b9f9",
            "max": 114,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_408fcd226c45469a944cb52b3ef73b70",
            "value": 114
          }
        },
        "3f5c0ecc39854b1a9e59b9819a1a004f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d87ba61bdd146f2b8ef8f63e6a09101",
            "placeholder": "​",
            "style": "IPY_MODEL_19b039bc15ed4981be05c2663d4a6afb",
            "value": " 114/114 [04:43&lt;00:00,  2.24s/it]"
          }
        },
        "dfcb3c129bb4474795b5ff39854676d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ec9a908a012446ab8b152bba9ccb7ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1d540f250754fb2b68a7a583ddc5ac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88966dffcdb746dd9f9e60a6d547b9f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "408fcd226c45469a944cb52b3ef73b70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d87ba61bdd146f2b8ef8f63e6a09101": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19b039bc15ed4981be05c2663d4a6afb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tabaraei/depression-detection/blob/master/baseline_replication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- They merge all the existing data, and perform a Kfold (k=3) to distribute train/test sets.\n",
        "- This code is designed to augment or resample data points based on certain conditions (whether an index is in audio_dep_idxs_tmp) by generating permutations of features and adding them to audio_features and corresponding labels (audio_targets). The augmented data points are then tracked using train_idxs. This approach likely aims to increase the diversity of training data by creating variations of existing data points."
      ],
      "metadata": {
        "id": "t58p0xF3dolt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "\n",
        "DATASET_DIR = '/content/drive/MyDrive/Data/DepressionDetection/EATD-Corpus'\n",
        "BASELINE_DIR = '/content/drive/MyDrive/Data/DepressionDetection/Baseline'\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LecNXSaQ3Erq",
        "outputId": "7d3a2232-21b4-42b8-fcde-8c16a7725eea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(f'{BASELINE_DIR}/Features/AudioWhole', exist_ok=True)\n",
        "os.makedirs(f'{BASELINE_DIR}/Features/TextWhole', exist_ok=True)\n",
        "os.makedirs(f'{BASELINE_DIR}/Features/TrainIdx', exist_ok=True)\n",
        "\n",
        "os.makedirs(f'{BASELINE_DIR}/Model/ClassificationWhole/Audio', exist_ok=True)\n",
        "os.makedirs(f'{BASELINE_DIR}/Model/ClassificationWhole/Text', exist_ok=True)\n",
        "os.makedirs(f'{BASELINE_DIR}/Model/ClassificationWhole/Fuse', exist_ok=True)\n",
        "os.makedirs(f'{BASELINE_DIR}/Model/ELMoForManyLangs', exist_ok=True)"
      ],
      "metadata": {
        "id": "IsQrVXicD5xB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path_to_remove_audio_features = f'{BASELINE_DIR}/Features/AudioWhole'\n",
        "# os.system(f'rm -rf {path_to_remove_audio_features}/*')\n",
        "\n",
        "# path_to_remove_text_features = f'{BASELINE_DIR}/Features/TextWhole'\n",
        "# os.system(f'rm -rf {path_to_remove_text_features}/*')\n",
        "\n",
        "path_to_remove_train_indecs = f'{BASELINE_DIR}/Features/TrainIdx'\n",
        "os.system(f'rm -rf {path_to_remove_train_indecs}/*')\n",
        "\n",
        "path_to_remove_trained_models = f'{BASELINE_DIR}/Model/ClassificationWhole'\n",
        "os.system(f'rm -rf {path_to_remove_trained_models}/*')"
      ],
      "metadata": {
        "id": "PF7qh4KxHE5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio"
      ],
      "metadata": {
        "id": "nIROExnrl8Ws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `NetVLAD`\n",
        "Used to enforce same-length audio features extracted as clusters"
      ],
      "metadata": {
        "id": "MJjTgX832YRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import tensorflow as tf\n",
        "# import tensorflow.contrib.slim as slim\n",
        "import numpy as np\n",
        "from keras import initializers, layers\n",
        "import keras.backend as K\n",
        "import sys"
      ],
      "metadata": {
        "id": "2OJgYzeH20vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NetVLAD(layers.Layer):\n",
        "    \"\"\"Creates a NetVLAD class.\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_size, max_samples, cluster_size, output_dim, **kwargs):\n",
        "\n",
        "        self.feature_size = feature_size\n",
        "        self.max_samples = max_samples\n",
        "        self.output_dim = output_dim\n",
        "        self.cluster_size = cluster_size\n",
        "        super(NetVLAD, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "    # Create a trainable weight variable for this layer.\n",
        "        self.cluster_weights = self.add_weight(name='kernel_W1',\n",
        "                                      shape=(self.feature_size, self.cluster_size),\n",
        "                                      initializer=tf.random_normal_initializer(stddev=1 / math.sqrt(self.feature_size)),\n",
        "                                      trainable=True)\n",
        "        self.cluster_biases = self.add_weight(name='kernel_B1',\n",
        "                                      shape=(self.cluster_size,),\n",
        "                                      initializer=tf.random_normal_initializer(stddev=1 / math.sqrt(self.feature_size)),\n",
        "                                      trainable=True)\n",
        "        self.cluster_weights2 = self.add_weight(name='kernel_W2',\n",
        "                                      shape=(1,self.feature_size, self.cluster_size),\n",
        "                                      initializer=tf.random_normal_initializer(stddev=1 / math.sqrt(self.feature_size)),\n",
        "                                      trainable=True)\n",
        "        self.hidden1_weights = self.add_weight(name='kernel_H1',\n",
        "                                      shape=(self.cluster_size*self.feature_size, self.output_dim),\n",
        "                                      initializer=tf.random_normal_initializer(stddev=1 / math.sqrt(self.cluster_size)),\n",
        "                                      trainable=True)\n",
        "\n",
        "        super(NetVLAD, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, reshaped_input):\n",
        "        \"\"\"Forward pass of a NetVLAD block.\n",
        "\n",
        "        Args:\n",
        "        reshaped_input: If your input is in that form:\n",
        "        'batch_size' x 'max_samples' x 'feature_size'\n",
        "        It should be reshaped in the following form:\n",
        "        'batch_size*max_samples' x 'feature_size'\n",
        "        by performing:\n",
        "        reshaped_input = tf.reshape(input, [-1, features_size])\n",
        "\n",
        "        Returns:\n",
        "        vlad: the pooled vector of size: 'batch_size' x 'output_dim'\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        In Keras, there are two way to do matrix multiplication (dot product)\n",
        "        1) K.dot : AxB -> when A has batchsize and B doesn't, use K.dot\n",
        "        2) tf.matmul: AxB -> when A and B both have batchsize, use tf.matmul\n",
        "\n",
        "        Error example: Use tf.matmul when A has batchsize (3 dim) and B doesn't (2 dim)\n",
        "        ValueError: Shape must be rank 2 but is rank 3 for 'net_vlad_1/MatMul' (op: 'MatMul') with input shapes: [?,21,64], [64,3]\n",
        "\n",
        "        tf.matmul might still work when the dim of A is (?,64), but this is too confusing.\n",
        "        Just follow the above rules.\n",
        "        \"\"\"\n",
        "        activation = K.dot(reshaped_input, self.cluster_weights)\n",
        "\n",
        "        activation += self.cluster_biases\n",
        "\n",
        "        activation = tf.nn.softmax(activation)\n",
        "\n",
        "        activation = tf.reshape(activation,\n",
        "                [-1, self.max_samples, self.cluster_size])\n",
        "\n",
        "        a_sum = tf.reduce_sum(activation,-2,keep_dims=True)\n",
        "\n",
        "        a = tf.multiply(a_sum,self.cluster_weights2)\n",
        "\n",
        "        activation = tf.transpose(activation,perm=[0,2,1])\n",
        "\n",
        "        reshaped_input = tf.reshape(reshaped_input,[-1,\n",
        "            self.max_samples, self.feature_size])\n",
        "\n",
        "        vlad = tf.matmul(activation,reshaped_input)\n",
        "        vlad = tf.transpose(vlad,perm=[0,2,1])\n",
        "        vlad = tf.subtract(vlad,a)\n",
        "        vlad = tf.nn.l2_normalize(vlad,1)\n",
        "        vlad = tf.reshape(vlad,[-1, self.cluster_size*self.feature_size])\n",
        "        vlad = tf.nn.l2_normalize(vlad,1)\n",
        "        vlad = K.dot(vlad, self.hidden1_weights)\n",
        "\n",
        "        return vlad\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.output_dim])"
      ],
      "metadata": {
        "id": "ewYLYHXw2ljN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `audio_features_whole.py`\n",
        "Extracts the audio features and stores them"
      ],
      "metadata": {
        "id": "5ITC9B9Sx1nZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wave\n",
        "import librosa\n",
        "# from python_speech_features import *\n",
        "import sys\n",
        "import pickle\n",
        "import tensorflow.compat.v1 as tf\n",
        "# import vggish.vggish_input as vggish_input\n",
        "# import vggish.vggish_params as vggish_params\n",
        "# import vggish.vggish_postprocess as vggish_postprocess\n",
        "# import vggish.vggish_slim as vggish_slim\n",
        "# import loupe_keras as lpk\n",
        "# from allennlp.commands.elmo import ElmoEmbedder\n",
        "from tqdm.notebook import trange, tqdm"
      ],
      "metadata": {
        "id": "XKhG1D4myDLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sys.path.append('/Users/linlin/Desktop/depression/classfication')\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "# elmo = ElmoEmbedder()\n",
        "\n",
        "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "# prefix = os.path.abspath(os.path.join(os.getcwd(), \".\"))\n",
        "\n",
        "# # Paths to downloaded VGGish files.\n",
        "# checkpoint_path =os.path.join(os.getcwd(),  'vggish/vggish_model.ckpt')\n",
        "# pca_params_path = os.path.join(os.getcwd(), 'vggish/vggish_pca_params.npz')\n",
        "\n",
        "cluster_size = 16\n",
        "min_len = 100\n",
        "max_len = -1"
      ],
      "metadata": {
        "id": "PhuQ6C29yATF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def to_vggish_embedds(x, sr):\n",
        "#     # x为输入的音频，sr为sample_rate\n",
        "#     input_batch = vggish_input.waveform_to_examples(x, sr)\n",
        "#     with tf.Graph().as_default(), tf.Session() as sess:\n",
        "#       vggish_slim.define_vggish_slim()\n",
        "#       vggish_slim.load_vggish_slim_checkpoint(sess, checkpoint_path)\n",
        "\n",
        "#       features_tensor = sess.graph.get_tensor_by_name(vggish_params.INPUT_TENSOR_NAME)\n",
        "#       embedding_tensor = sess.graph.get_tensor_by_name(vggish_params.OUTPUT_TENSOR_NAME)\n",
        "#       [embedding_batch] = sess.run([embedding_tensor],\n",
        "#                                    feed_dict={features_tensor: input_batch})\n",
        "\n",
        "#     # Postprocess the results to produce whitened quantized embeddings.\n",
        "#     pproc = vggish_postprocess.Postprocessor(pca_params_path)\n",
        "#     postprocessed_batch = pproc.postprocess(embedding_batch)\n",
        "\n",
        "#     return tf.cast(postprocessed_batch, dtype='float32')"
      ],
      "metadata": {
        "id": "Jv8hHK__yFcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wav2vlad(wave_data, sr):\n",
        "    global cluster_size\n",
        "    signal = wave_data\n",
        "    melspec = librosa.feature.melspectrogram(y=signal, n_mels=80,sr=sr).astype(np.float32).T\n",
        "    melspec = np.log(np.maximum(1e-6, melspec))\n",
        "    feature_size = melspec.shape[1]\n",
        "    max_samples = melspec.shape[0]\n",
        "    output_dim = cluster_size * 16\n",
        "    feat = NetVLAD(feature_size=feature_size, max_samples=max_samples, \\\n",
        "                            cluster_size=cluster_size, output_dim=output_dim) \\\n",
        "                                (tf.convert_to_tensor(melspec))\n",
        "    with tf.Session() as sess:\n",
        "        init = tf.global_variables_initializer()\n",
        "        sess.run(init)\n",
        "        r = feat.numpy()\n",
        "    return r"
      ],
      "metadata": {
        "id": "5BimqtP7yI50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(audio_features, targets, folder):\n",
        "    global max_len, min_len\n",
        "    if not os.path.exists(f'{DATASET_DIR}/{folder}/positive_out.wav'):\n",
        "        return\n",
        "    positive_file = wave.open(f'{DATASET_DIR}/{folder}/positive_out.wav')\n",
        "    sr1 = positive_file.getframerate()\n",
        "    nframes1 = positive_file.getnframes()\n",
        "    wave_data1 = np.frombuffer(positive_file.readframes(nframes1), dtype=np.short).astype(float)\n",
        "    len1 = nframes1 / sr1\n",
        "\n",
        "    neutral_file = wave.open(f'{DATASET_DIR}/{folder}/neutral_out.wav')\n",
        "    sr2 = neutral_file.getframerate()\n",
        "    nframes2 = neutral_file.getnframes()\n",
        "    wave_data2 = np.frombuffer(neutral_file.readframes(nframes2), dtype=np.short).astype(float)\n",
        "    len2 = nframes2 / sr2\n",
        "\n",
        "    negative_file = wave.open(f'{DATASET_DIR}/{folder}/negative_out.wav')\n",
        "    sr3 = negative_file.getframerate()\n",
        "    nframes3 = negative_file.getnframes()\n",
        "    wave_data3 = np.frombuffer(negative_file.readframes(nframes3), dtype=np.short).astype(float)\n",
        "    len3 = nframes3/sr3\n",
        "\n",
        "    for l in [len1, len2, len3]:\n",
        "        if l > max_len:\n",
        "            max_len = l\n",
        "        if l < min_len:\n",
        "            min_len = l\n",
        "\n",
        "    with open(f'{DATASET_DIR}/{folder}/new_label.txt') as fli:\n",
        "        target = float(fli.readline())\n",
        "\n",
        "    if wave_data1.shape[0] < 1:\n",
        "        wave_data1 = np.array([1e-4]*sr1*5)\n",
        "    if wave_data2.shape[0] < 1:\n",
        "        wave_data2 = np.array([1e-4]*sr2*5)\n",
        "    if wave_data3.shape[0] < 1:\n",
        "        wave_data3 = np.array([1e-4]*sr3*5)\n",
        "    audio_features.append([wav2vlad(wave_data1, sr1), wav2vlad(wave_data2, sr2), \\\n",
        "        wav2vlad(wave_data3, sr3)])\n",
        "    targets.append(1 if target >= 53 else 0)\n",
        "    # targets.append(target)"
      ],
      "metadata": {
        "id": "Iq2MxbheyMtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_features = []\n",
        "audio_targets = []\n",
        "\n",
        "for index in trange(114):\n",
        "    extract_features(audio_features, audio_targets, f't_{index+1}')\n",
        "\n",
        "for index in trange(114):\n",
        "    extract_features(audio_features, audio_targets, f'v_{index+1}')"
      ],
      "metadata": {
        "id": "m5Zs9wf33IUS",
        "outputId": "924d8c0f-426b-484b-9cb0-81a32f948ae9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "59a6aa0154074fba8eaa223ae334a857",
            "c0e93bc0da3e47d5909ee27c2f30505d",
            "16ad6b8178554a4ab7dd7aaaf97799f0",
            "76ed8d04db9b4039b55b4c0efb2da419",
            "d38aa751fefc4c68913bb7fc32bb4c42",
            "56e704b9bf4440628c5daf2f36fe5ac3",
            "67e01c8a850846da8eac2d0d34ad12d6",
            "0dc820ffde0e467e92068b3c278ce33e",
            "26bad87aaa9248a2a3ec7066626b262a",
            "02c5a0c6465b41c99e96d89dc7aa2b95",
            "4d723aef81d34be4a98d85b3b112942b",
            "6a38ecf4614048c68b1102fb3f7b87a1",
            "a8b0f05ba2cc442e9ef0816a43938218",
            "7f592e7f16e045c894d4867a1460af5d",
            "f9999eaa4571419d83af11480ba03462",
            "992574e032514d43b3d662d1ade38217",
            "d1453550fc384d048189b3e9e05868a3",
            "cff82e6f04534990afb25c2c53f1f285",
            "65556c90e11346739f1946885dca1865",
            "a9691c61c0f942fa89eb2a022694aa48",
            "b3d7531b353349deae20ce864c8a3b99",
            "1949b10a15924d789acddeda2e675f2b"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/114 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59a6aa0154074fba8eaa223ae334a857"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/114 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a38ecf4614048c68b1102fb3f7b87a1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Saving npz file locally...\")\n",
        "np.savez(f'{BASELINE_DIR}/Features/AudioWhole/whole_samples_clf_{cluster_size*16}.npz', audio_features)\n",
        "np.savez(f'{BASELINE_DIR}/Features/AudioWhole/whole_labels_clf_{cluster_size*16}.npz', audio_targets)\n",
        "\n",
        "print(max_len, min_len)"
      ],
      "metadata": {
        "id": "_ltZj3Og70vU",
        "outputId": "1ebaa825-0ce1-4b00-e931-0d75f7f2c513",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving npz file locally...\n",
            "111.02 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(audio_features), np.array(audio_features[0]).shape"
      ],
      "metadata": {
        "id": "mTIwtawa-nnL",
        "outputId": "9b4348da-746b-4294-89c2-45e5a4838d64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(162, (3, 1, 256))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `audio_gru_whole.py`\n",
        "The main learning algorithm and network architecture of BiLSTM for audio"
      ],
      "metadata": {
        "id": "4FCt050ZBfg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import itertools\n",
        "from tqdm.notebook import trange, tqdm"
      ],
      "metadata": {
        "id": "h1M_irA2DySP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_features = np.squeeze(np.load(f'{BASELINE_DIR}/Features/AudioWhole/whole_samples_clf_256.npz')['arr_0'], axis=2)\n",
        "audio_targets = np.load(f'{BASELINE_DIR}/Features/AudioWhole/whole_labels_clf_256.npz')['arr_0']\n",
        "audio_dep_idxs_tmp = np.where(audio_targets == 1)[0]\n",
        "audio_non_idxs = np.where(audio_targets == 0)[0]\n",
        "audio_features.shape, audio_targets.shape"
      ],
      "metadata": {
        "id": "w3Jx_J5JD0qh",
        "outputId": "3bf45c85-2f06-4d19-ec75-f20f967ca3f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((162, 3, 256), (162,))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioBiLSTM(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(AudioBiLSTM, self).__init__()\n",
        "        self.num_classes = config['num_classes']\n",
        "        self.learning_rate = config['learning_rate']\n",
        "        self.dropout = config['dropout']\n",
        "        self.hidden_dims = config['hidden_dims']\n",
        "        self.rnn_layers = config['rnn_layers']\n",
        "        self.embedding_size = config['embedding_size']\n",
        "        self.bidirectional = config['bidirectional']\n",
        "\n",
        "        self.build_model()\n",
        "        # self.init_weight()\n",
        "\n",
        "    def init_weight(net):\n",
        "        for name, param in net.named_parameters():\n",
        "            if not 'ln' in name:\n",
        "                if 'bias' in name:\n",
        "                    nn.init.constant_(param, 0.0)\n",
        "                elif 'weight' in name:\n",
        "                    nn.init.xavier_uniform_(param)\n",
        "\n",
        "    def build_model(self):\n",
        "        # attention layer\n",
        "        self.attention_layer = nn.Sequential(\n",
        "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
        "            nn.ReLU(inplace=True))\n",
        "        # self.attention_weights = self.attention_weights.view(self.hidden_dims, 1)\n",
        "\n",
        "        # self.lstm_net_audio = nn.LSTM(self.embedding_size,\n",
        "        #                         self.hidden_dims,\n",
        "        #                         num_layers=self.rnn_layers,\n",
        "        #                         dropout=self.dropout,\n",
        "        #                         bidirectional=self.bidirectional,\n",
        "        #                         batch_first=True)\n",
        "        self.lstm_net_audio = nn.GRU(self.embedding_size, self.hidden_dims,\n",
        "                                num_layers=self.rnn_layers, dropout=self.dropout, batch_first=True)\n",
        "\n",
        "        self.ln = nn.LayerNorm(self.embedding_size)\n",
        "\n",
        "        # FC层\n",
        "        self.fc_audio = nn.Sequential(\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.hidden_dims, self.num_classes),\n",
        "            # nn.ReLU(),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
        "        '''\n",
        "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
        "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
        "        :return: [batch_size, n_hidden]\n",
        "        '''\n",
        "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
        "        # h [batch_size, time_step, hidden_dims]\n",
        "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
        "        #         h = lstm_out\n",
        "        # [batch_size, num_layers * num_directions, n_hidden]\n",
        "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
        "        # [batch_size, 1, n_hidden]\n",
        "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
        "        # atten_w [batch_size, 1, hidden_dims]\n",
        "        atten_w = self.attention_layer(lstm_hidden)\n",
        "        # m [batch_size, time_step, hidden_dims]\n",
        "        m = nn.Tanh()(h)\n",
        "        # atten_context [batch_size, 1, time_step]\n",
        "       # print(atten_w.shape, m.transpose(1, 2).shape)\n",
        "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
        "        # softmax_w [batch_size, 1, time_step]\n",
        "        softmax_w = F.softmax(atten_context, dim=-1)\n",
        "        # context [batch_size, 1, hidden_dims]\n",
        "        context = torch.bmm(softmax_w, h)\n",
        "        result = context.squeeze(1)\n",
        "        return result\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ln(x)\n",
        "        x, _ = self.lstm_net_audio(x)\n",
        "        x = x.mean(dim=1)\n",
        "        out = self.fc_audio(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "q0FdDXtDD7oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'num_classes': 2,\n",
        "    'dropout': 0.5,\n",
        "    'rnn_layers': 2,\n",
        "    'embedding_size': 256,\n",
        "    'batch_size': 8,\n",
        "    'epochs': 170,\n",
        "    'learning_rate': 6e-6,\n",
        "    'hidden_dims': 256,\n",
        "    'bidirectional': False,\n",
        "    'cuda': False\n",
        "}"
      ],
      "metadata": {
        "id": "65RKaAA0HHgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save(model, filename):\n",
        "    save_filename = '{}.pt'.format(filename)\n",
        "    torch.save(model, save_filename)\n",
        "    print('Saved as %s' % save_filename)\n",
        "\n",
        "def standard_confusion_matrix(y_test, y_test_pred):\n",
        "    \"\"\"\n",
        "    Make confusion matrix with format:\n",
        "                  -----------\n",
        "                  | TP | FP |\n",
        "                  -----------\n",
        "                  | FN | TN |\n",
        "                  -----------\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : ndarray - 1D\n",
        "    y_pred : ndarray - 1D\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray - 2D\n",
        "    \"\"\"\n",
        "    [[tn, fp], [fn, tp]] = confusion_matrix(y_test.cpu().numpy(), y_test_pred)\n",
        "    return np.array([[tp, fp], [fn, tn]])\n",
        "\n",
        "def model_performance(y_test, y_test_pred_proba):\n",
        "    \"\"\"\n",
        "    Evaluation metrics for network performance.\n",
        "    \"\"\"\n",
        "    y_test_pred = y_test_pred_proba.data.max(1, keepdim=True)[1]\n",
        "\n",
        "    # Computing confusion matrix for test dataset\n",
        "    conf_matrix = standard_confusion_matrix(y_test, y_test_pred.numpy())\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    return y_test_pred, conf_matrix\n",
        "\n",
        "def train(epoch, train_idxs):\n",
        "    global lr, train_acc\n",
        "    model.train()\n",
        "    batch_idx = 1\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    pred = np.array([])\n",
        "    X_train = audio_features[train_idxs]\n",
        "    Y_train = audio_targets[train_idxs]\n",
        "    for i in range(0, X_train.shape[0], config['batch_size']):\n",
        "        if i + config['batch_size'] > X_train.shape[0]:\n",
        "            x, y = X_train[i:], Y_train[i:]\n",
        "        else:\n",
        "            x, y = X_train[i:(i + config['batch_size'])], Y_train[i:(\n",
        "                i + config['batch_size'])]\n",
        "        if config['cuda']:\n",
        "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True).cuda(), Variable(torch.from_numpy(y)).cuda()\n",
        "        else:\n",
        "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True), \\\n",
        "                Variable(torch.from_numpy(y))\n",
        "\n",
        "        # 将模型的参数梯度设置为0\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        #print(pred.shape, y.shape)\n",
        "        correct += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
        "        loss = criterion(output, y)\n",
        "        # 后向传播调整参数\n",
        "        loss.backward()\n",
        "        # 根据梯度更新网络参数\n",
        "        optimizer.step()\n",
        "        batch_idx += 1\n",
        "        # loss.item()能够得到张量中的元素值\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    train_acc = correct\n",
        "    print(\n",
        "        'Train Epoch: {:2d}\\t Learning rate: {:.4f}\\tLoss: {:.6f}\\t Accuracy: {}/{} ({:.0f}%)\\n '\n",
        "        .format(epoch + 1, config['learning_rate'], total_loss, correct,\n",
        "                X_train.shape[0], 100. * correct / X_train.shape[0]))\n",
        "\n",
        "\n",
        "def evaluate(model, test_idxs, fold, train_idxs_tmp, train_idxs):\n",
        "    model.eval()\n",
        "    batch_idx = 1\n",
        "    total_loss = 0\n",
        "    global max_f1, max_acc, min_mae, X_test_lens, max_prec, max_rec\n",
        "    pred = np.array([])\n",
        "    with torch.no_grad():\n",
        "        if config['cuda']:\n",
        "            x, y = Variable(torch.from_numpy(audio_features[test_idxs]).type(torch.FloatTensor), requires_grad=True).cuda(),\\\n",
        "                Variable(torch.from_numpy(audio_targets[test_idxs])).cuda()\n",
        "        else:\n",
        "            x, y = Variable(torch.from_numpy(audio_features[test_idxs]).type(torch.FloatTensor), requires_grad=True), \\\n",
        "                Variable(torch.from_numpy(audio_targets[test_idxs])).type(torch.LongTensor)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        y_test_pred, conf_matrix = model_performance(y, output.cpu())\n",
        "        accuracy = float(conf_matrix[0][0] + conf_matrix[1][1]) / np.sum(conf_matrix)\n",
        "        precision = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[0][1])\n",
        "        recall = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[1][0])\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "        print(\"Accuracy: {}\".format(accuracy))\n",
        "        print(\"Precision: {}\".format(precision))\n",
        "        print(\"Recall: {}\".format(recall))\n",
        "        print(\"F1-Score: {}\\n\".format(f1_score))\n",
        "        print('=' * 89)\n",
        "\n",
        "        if max_f1 <= f1_score and train_acc > len(train_idxs)*0.90  and f1_score > 0.5:\n",
        "            max_f1 = f1_score\n",
        "            max_acc = accuracy\n",
        "            max_rec = recall\n",
        "            max_prec = precision\n",
        "            mode ='gru'\n",
        "            save(model, f\"{BASELINE_DIR}/Model/ClassificationWhole/Audio/BiLSTM_{mode}_vlad{config['embedding_size']}_{config['hidden_dims']}_{max_f1:.2f}_{fold}\")\n",
        "            np.save(f'{BASELINE_DIR}/Features/TrainIdx/train_idxs_{f1_score:.2f}_{fold}.npy', train_idxs_tmp)\n",
        "            print('*' * 64)\n",
        "            print('model saved: f1: {}\\tacc: {}'.format(max_f1, max_acc))\n",
        "            print('*' * 64)\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "def get_param_group(model):\n",
        "    nd_list = []\n",
        "    param_list = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'ln' in name:\n",
        "            nd_list.append(param)\n",
        "        else:\n",
        "            param_list.append(param)\n",
        "    return [{'params': param_list, 'weight_decay': 1e-5}, {'params': nd_list, 'weight_decay': 0}]"
      ],
      "metadata": {
        "id": "f8ezUxy-Bhrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=3, shuffle=True)\n",
        "fold = 1\n",
        "for train_idxs_tmp, test_idxs_tmp in kf.split(audio_features):\n",
        "# train_idxs_tmps = [\n",
        "#     np.load(f'{BASELINE_DIR}/Features/TrainIdx/train_idxs_0.63_1.npy', allow_pickle=True),\n",
        "#     np.load(f'{BASELINE_DIR}/Features/TrainIdx/train_idxs_0.60_2.npy', allow_pickle=True),\n",
        "#     np.load(f'{BASELINE_DIR}/Features/TrainIdx/train_idxs_0.60_3.npy', allow_pickle=True)\n",
        "# ]\n",
        "# for idx_idx, train_idxs_tmp in enumerate(train_idxs_tmps):\n",
        "#     fold = idx_idx + 1\n",
        "#     # if idx_idx != 1:\n",
        "#     #     continue\n",
        "#     test_idxs_tmp = list(set(list(audio_dep_idxs_tmp)+list(audio_non_idxs)) - set(train_idxs_tmp))\n",
        "    train_idxs, test_idxs = [], []\n",
        "    resample_idxs = [0,1,2,3,4,5]\n",
        "    # depression data augmentation\n",
        "    for idx in train_idxs_tmp:\n",
        "        if idx in audio_dep_idxs_tmp:\n",
        "            feat = audio_features[idx]\n",
        "            count = 0\n",
        "            for i in itertools.permutations(feat, feat.shape[0]):\n",
        "                if count in resample_idxs:\n",
        "                    audio_features = np.vstack((audio_features, np.expand_dims(list(i), 0)))\n",
        "                    audio_targets = np.hstack((audio_targets, 1))\n",
        "                    train_idxs.append(len(audio_features)-1)\n",
        "                count += 1\n",
        "        else:\n",
        "            train_idxs.append(idx)\n",
        "\n",
        "    for idx in test_idxs_tmp:\n",
        "        if idx in audio_dep_idxs_tmp:\n",
        "            feat = audio_features[idx]\n",
        "            count = 0\n",
        "            # resample_idxs = random.sample(range(6), 4)\n",
        "            resample_idxs = [0,1,4,5]\n",
        "            for i in itertools.permutations(feat, feat.shape[0]):\n",
        "                if count in resample_idxs:\n",
        "                    audio_features = np.vstack((audio_features, np.expand_dims(list(i), 0)))\n",
        "                    audio_targets = np.hstack((audio_targets, 1))\n",
        "                    test_idxs.append(len(audio_features)-1)\n",
        "                count += 1\n",
        "        else:\n",
        "            test_idxs.append(idx)\n",
        "        # test_idxs.append(idx)\n",
        "\n",
        "    model = AudioBiLSTM(config)\n",
        "\n",
        "    if config['cuda']:\n",
        "        model = model.cuda()\n",
        "\n",
        "    param_group = get_param_group(model)\n",
        "    optimizer = optim.AdamW(param_group, lr=config['learning_rate'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # criterion = FocalLoss(class_num=2)\n",
        "    max_f1 = -1\n",
        "    max_acc = -1\n",
        "    max_rec = -1\n",
        "    max_prec = -1\n",
        "    train_acc = -1\n",
        "\n",
        "    for ep in trange(1, config['epochs']):\n",
        "        train(ep, train_idxs)\n",
        "        tloss = evaluate(model, test_idxs, fold, train_idxs_tmp, train_idxs)\n",
        "    fold += 1"
      ],
      "metadata": {
        "id": "Vtbb_0C6HPfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `AudioModelChecking.py`\n",
        "Only loads the previously learnt \"BiLSTM_gru_vlad256_256_0\" models and reports the performances"
      ],
      "metadata": {
        "id": "X8ba6rgbfB6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wave\n",
        "import re\n",
        "import os\n",
        "import tensorflow.compat.v1 as tf\n",
        "import random\n",
        "import itertools\n",
        "# from audio_gru_whole import AudioBiLSTM\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle"
      ],
      "metadata": {
        "id": "RnvAO8bsfS-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, rnn_layers, dropout, num_classes, audio_hidden_dims, audio_embed_size):\n",
        "        super(BiLSTM, self).__init__()\n",
        "\n",
        "        self.lstm_net_audio = nn.GRU(audio_embed_size, audio_hidden_dims,\n",
        "                                num_layers=rnn_layers, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.fc_audio = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(audio_hidden_dims, audio_hidden_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(audio_hidden_dims, num_classes),\n",
        "            # nn.ReLU(),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm_net_audio(x)\n",
        "        # x = self.bn(x)\n",
        "        x = x.sum(dim=1)\n",
        "        out = self.fc_audio(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "A6YPZJsdfF7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prefix = os.path.abspath(os.path.join(os.getcwd(), \".\"))\n",
        "# audio_features = np.squeeze(np.load(os.path.join(prefix, 'Features/Audio/whole_samples_clf_avid256.npz'))['arr_0'], axis=2)\n",
        "# audio_targets = np.load(os.path.join(prefix, 'Features/Audio/whole_labels_clf_avid256.npz'))['arr_0']\n",
        "\n",
        "audio_features = np.squeeze(np.load(f'{BASELINE_DIR}/Features/AudioWhole/whole_samples_clf_256.npz')['arr_0'], axis=2)\n",
        "audio_targets = np.load(f'{BASELINE_DIR}/Features/AudioWhole/whole_labels_clf_256.npz')['arr_0']\n",
        "\n",
        "audio_dep_idxs = np.where(audio_targets == 1)[0]\n",
        "audio_non_idxs = np.where(audio_targets == 0)[0]"
      ],
      "metadata": {
        "id": "PvgpZHsogHXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standard_confusion_matrix(y_test, y_test_pred):\n",
        "    \"\"\"\n",
        "    Make confusion matrix with format:\n",
        "                  -----------\n",
        "                  | TP | FP |\n",
        "                  -----------\n",
        "                  | FN | TN |\n",
        "                  -----------\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : ndarray - 1D\n",
        "    y_pred : ndarray - 1D\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray - 2D\n",
        "    \"\"\"\n",
        "    [[tn, fp], [fn, tp]] = confusion_matrix(y_test, y_test_pred)\n",
        "    return np.array([[tp, fp], [fn, tn]])\n",
        "\n",
        "def model_performance(y_test, y_test_pred_proba):\n",
        "    \"\"\"\n",
        "    Evaluation metrics for network performance.\n",
        "    \"\"\"\n",
        "    # y_test_pred = y_test_pred_proba.data.max(1, keepdim=True)[1]\n",
        "    y_test_pred = y_test_pred_proba\n",
        "\n",
        "    # Computing confusion matrix for test dataset\n",
        "    conf_matrix = standard_confusion_matrix(y_test, y_test_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    return y_test_pred, conf_matrix"
      ],
      "metadata": {
        "id": "d0N2R13egD3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'num_classes': 2,\n",
        "    'dropout': 0.5,\n",
        "    'rnn_layers': 2,\n",
        "    'embedding_size': 256,\n",
        "    'batch_size': 4,\n",
        "    'epochs': 100,\n",
        "    'learning_rate': 1e-5,\n",
        "    'hidden_dims': 256,\n",
        "    'bidirectional': False,\n",
        "    'cuda': False\n",
        "}"
      ],
      "metadata": {
        "id": "I-KBYLpJf-41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# audio_lstm_model = torch.load(os.path.join(prefix, 'Model/Classification/Audio/BiLSTM_gru_vlad256_256_0.80.pt'))\n",
        "# audio_lstm_model = torch.load(os.path.join(prefix, 'Model/Classification/Audio3/BiLSTM_gru_vlad256_256_0.89.pt'))\n",
        "# audio_lstm_model = torch.load(os.path.join(prefix, 'Model/Classification/Audio2/BiLSTM_gru_vlad256_256_0.65.pt'))\n",
        "\n",
        "# model = BiLSTM(config['rnn_layers'], config['dropout'], config['num_classes'], \\\n",
        "#          config['hidden_dims'], config['embedding_size'])\n",
        "\n",
        "# model_state_dict = {}\n",
        "# model_state_dict['lstm_net_audio.weight_ih_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_ih_l0']\n",
        "# model_state_dict['lstm_net_audio.weight_hh_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_hh_l0']\n",
        "# model_state_dict['lstm_net_audio.bias_ih_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_ih_l0']\n",
        "# model_state_dict['lstm_net_audio.bias_hh_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_hh_l0']\n",
        "\n",
        "# model_state_dict['lstm_net_audio.weight_ih_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_ih_l1']\n",
        "# model_state_dict['lstm_net_audio.weight_hh_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_hh_l1']\n",
        "# model_state_dict['lstm_net_audio.bias_ih_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_ih_l1']\n",
        "# model_state_dict['lstm_net_audio.bias_hh_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_hh_l1']\n",
        "\n",
        "# model_state_dict['fc_audio.1.weight'] = audio_lstm_model.state_dict()['fc_audio.1.weight']\n",
        "# model_state_dict['fc_audio.1.bias'] = audio_lstm_model.state_dict()['fc_audio.1.bias']\n",
        "# model_state_dict['fc_audio.4.weight'] = audio_lstm_model.state_dict()['fc_audio.4.weight']\n",
        "# model_state_dict['fc_audio.4.bias'] = audio_lstm_model.state_dict()['fc_audio.4.bias']\n",
        "# model_state_dict = audio_lstm_model.state_dict()\n",
        "# model.load_state_dict(model_state_dict, strict=False)"
      ],
      "metadata": {
        "id": "B15wRLmygA73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_idxs):\n",
        "    model.eval()\n",
        "    batch_idx = 1\n",
        "    total_loss = 0\n",
        "    pred = torch.empty(config['batch_size'], 1).type(torch.LongTensor)\n",
        "    # X_test = audio_features[test_dep_idxs+test_non_idxs]\n",
        "    # Y_test = audio_targets[test_dep_idxs+test_non_idxs]\n",
        "    X_test = audio_features[test_idxs]\n",
        "    Y_test = audio_targets[test_idxs]\n",
        "    global max_train_acc, max_acc,max_f1\n",
        "    for i in range(0, X_test.shape[0], config['batch_size']):\n",
        "        if i + config['batch_size'] > X_test.shape[0]:\n",
        "            x, y = X_test[i:], Y_test[i:]\n",
        "        else:\n",
        "            x, y = X_test[i:(i+config['batch_size'])], Y_test[i:(i+config['batch_size'])]\n",
        "        if config['cuda']:\n",
        "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True).cuda(), Variable(torch.from_numpy(y)).cuda()\n",
        "        else:\n",
        "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True), Variable(torch.from_numpy(y))\n",
        "        with torch.no_grad():\n",
        "            output = model(x.squeeze(2))\n",
        "        pred = torch.cat((pred, output.data.max(1, keepdim=True)[1]))\n",
        "\n",
        "    y_test_pred, conf_matrix = model_performance(Y_test, pred[config['batch_size']:])\n",
        "    print('Calculating additional test metrics...')\n",
        "    accuracy = float(conf_matrix[0][0] + conf_matrix[1][1]) / np.sum(conf_matrix)\n",
        "    precision = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[0][1])\n",
        "    recall = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[1][0])\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    print(\"Accuracy: {}\".format(accuracy))\n",
        "    print(\"Precision: {}\".format(precision))\n",
        "    print(\"Recall: {}\".format(recall))\n",
        "    print(\"F1-Score: {}\\n\".format(f1_score))\n",
        "    print('='*89)\n",
        "    return precision, recall, f1_score"
      ],
      "metadata": {
        "id": "iBF12dBcf6LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate(audio_features_test, fuse_targets_test, audio_lstm_model)\n",
        "# evaluate(model)\n",
        "\n",
        "idxs_paths = ['train_idxs_0.63_1.npy', 'train_idxs_0.65_2.npy', 'train_idxs_0.60_3.npy']\n",
        "audio_model_paths = ['BiLSTM_gru_vlad256_256_0.67_1.pt', 'BiLSTM_gru_vlad256_256_0.67_2.pt', 'BiLSTM_gru_vlad256_256_0.63_3.pt']\n",
        "ps, rs, fs = [], [], []\n",
        "for fold in range(3):\n",
        "    train_idxs_tmp = np.load(f'{BASELINE_DIR}/Features/TrainIdx/{idxs_paths[fold]}', allow_pickle=True)\n",
        "    test_idxs_tmp = list(set(list(audio_dep_idxs)+list(audio_non_idxs)) - set(train_idxs_tmp))\n",
        "    audio_lstm_model = torch.load(f'{BASELINE_DIR}/Model/ClassificationWhole/Audio/{audio_model_paths[fold]}')\n",
        "\n",
        "    train_idxs, test_idxs = [], []\n",
        "    for idx in train_idxs_tmp:\n",
        "        if idx in audio_dep_idxs:\n",
        "            feat = audio_features[idx]\n",
        "            count = 0\n",
        "            resample_idxs = [0,1,2,3,4,5]\n",
        "            for i in itertools.permutations(feat, feat.shape[0]):\n",
        "                if count in resample_idxs:\n",
        "                    audio_features = np.vstack((audio_features, np.expand_dims(list(i), 0)))\n",
        "                    audio_targets = np.hstack((audio_targets, 1))\n",
        "                    train_idxs.append(len(audio_features)-1)\n",
        "                count += 1\n",
        "        else:\n",
        "            train_idxs.append(idx)\n",
        "\n",
        "    for idx in test_idxs_tmp:\n",
        "        if idx in audio_dep_idxs:\n",
        "            feat = audio_features[idx]\n",
        "            count = 0\n",
        "            # resample_idxs = random.sample(range(6), 4)\n",
        "            resample_idxs = [0,1,4,5]\n",
        "            for i in itertools.permutations(feat, feat.shape[0]):\n",
        "                if count in resample_idxs:\n",
        "                    audio_features = np.vstack((audio_features, np.expand_dims(list(i), 0)))\n",
        "                    audio_targets = np.hstack((audio_targets, 1))\n",
        "                    test_idxs.append(len(audio_features)-1)\n",
        "                count += 1\n",
        "        else:\n",
        "            test_idxs.append(idx)\n",
        "    p, r, f = evaluate(audio_lstm_model, test_idxs)\n",
        "    ps.append(p)\n",
        "    rs.append(r)\n",
        "    fs.append(f)\n",
        "print('precison: {} \\n recall: {} \\n f1 score: {}'.format(np.mean(ps), np.mean(rs), np.mean(fs)))"
      ],
      "metadata": {
        "id": "eIAaXF2hf3fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `AudioTraditionalClassifiers.py`\n",
        "Tries \"DecisionTreeClassifier\", \"LogisticRegression\", \"SVC\", and \"RandomForestClassifier\" on the same folds to compare against the proposed GRU"
      ],
      "metadata": {
        "id": "9bS_G1fWiUKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "u7fEDu6gjbFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_features = np.squeeze(np.load(f'{BASELINE_DIR}/Features/AudioWhole/whole_samples_clf_256.npz')['arr_0'], axis=2)\n",
        "audio_targets = np.load(f'{BASELINE_DIR}/Features/AudioWhole/whole_labels_clf_256.npz')['arr_0']\n",
        "\n",
        "audio_dep_idxs_tmp = np.where(audio_targets == 1)[0]\n",
        "audio_non_idxs = np.where(audio_targets == 0)[0]"
      ],
      "metadata": {
        "id": "5zcUzjFJjc4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_performance(y_test, y_test_pred_proba):\n",
        "    \"\"\"\n",
        "    Evaluation metrics for network performance.\n",
        "    \"\"\"\n",
        "#     y_test_pred = y_test_pred_proba.data.max(1, keepdim=True)[1]\n",
        "    y_test_pred = y_test_pred_proba\n",
        "\n",
        "    # Computing confusion matrix for test dataset\n",
        "    conf_matrix = standard_confusion_matrix(y_test, y_test_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    return y_test_pred, conf_matrix\n",
        "\n",
        "def standard_confusion_matrix(y_test, y_test_pred):\n",
        "    [[tn, fp], [fn, tp]] = confusion_matrix(y_test, y_test_pred)\n",
        "    return np.array([[tp, fp], [fn, tn]])"
      ],
      "metadata": {
        "id": "4eSr9t0Cjfns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_idxs_tmps = [\n",
        "    np.load(f'{BASELINE_DIR}/Features/TrainIdx/train_idxs_0.63_1.npy', allow_pickle=True),\n",
        "    np.load(f'{BASELINE_DIR}/Features/TrainIdx/train_idxs_0.65_2.npy', allow_pickle=True),\n",
        "    np.load(f'{BASELINE_DIR}/Features/TrainIdx/train_idxs_0.60_3.npy', allow_pickle=True)\n",
        "]\n",
        "precs, recs, f1s = [], [], []\n",
        "for idx_idx, train_idxs_tmp in enumerate(train_idxs_tmps):\n",
        "    test_idxs_tmp = list(set(list(audio_dep_idxs_tmp)+list(audio_non_idxs)) - set(train_idxs_tmp))\n",
        "    train_idxs, test_idxs = [], []\n",
        "    # depression data augmentation\n",
        "    for idx in train_idxs_tmp:\n",
        "        if idx in audio_dep_idxs_tmp:\n",
        "            feat = audio_features[idx]\n",
        "            count = 0\n",
        "            resample_idxs = [0,1,2,3,4,5]\n",
        "            for i in itertools.permutations(feat, feat.shape[0]):\n",
        "                if count in resample_idxs:\n",
        "                    audio_features = np.vstack((audio_features, np.expand_dims(list(i), 0)))\n",
        "                    audio_targets = np.hstack((audio_targets, 1))\n",
        "                    train_idxs.append(len(audio_features)-1)\n",
        "                count += 1\n",
        "        else:\n",
        "            train_idxs.append(idx)\n",
        "\n",
        "    for idx in test_idxs_tmp:\n",
        "        if idx in audio_dep_idxs_tmp:\n",
        "            feat = audio_features[idx]\n",
        "            count = 0\n",
        "            # resample_idxs = random.sample(range(6), 4)\n",
        "            resample_idxs = [0,1,4,5]\n",
        "            for i in itertools.permutations(feat, feat.shape[0]):\n",
        "                if count in resample_idxs:\n",
        "                    audio_features = np.vstack((audio_features, np.expand_dims(list(i), 0)))\n",
        "                    audio_targets = np.hstack((audio_targets, 1))\n",
        "                    test_idxs.append(len(audio_features)-1)\n",
        "                count += 1\n",
        "        else:\n",
        "            test_idxs.append(idx)\n",
        "\n",
        "    X_train = audio_features[train_idxs]\n",
        "    Y_train = audio_targets[train_idxs]\n",
        "    X_test = audio_features[test_idxs]\n",
        "    Y_test = audio_targets[test_idxs]\n",
        "\n",
        "    # Decision Tree\n",
        "    # from sklearn import tree\n",
        "    # clf = tree.DecisionTreeClassifier(max_depth=20)\n",
        "\n",
        "    # svm\n",
        "    # from sklearn.svm import SVC\n",
        "    # clf = SVC(kernel='sigmoid')\n",
        "\n",
        "    # rf\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    clf = RandomForestClassifier(n_estimators=50)\n",
        "\n",
        "    # lr\n",
        "    # from sklearn.linear_model import LogisticRegression\n",
        "    # clf = LogisticRegression(solver='newton-cg')\n",
        "\n",
        "    clf.fit([f.flatten() for f in X_train], Y_train)\n",
        "    pred = clf.predict([f.flatten() for f in X_test])\n",
        "    # clf.fit([f.sum(axis=0) for f in X_train], Y_train)\n",
        "    # pred = clf.predict([f.sum(axis=0) for f in X_test])\n",
        "\n",
        "    y_test_pred, conf_matrix = model_performance(Y_test, pred)\n",
        "\n",
        "    # custom evaluation metrics\n",
        "    print('Calculating additional test metrics...')\n",
        "    accuracy = float(conf_matrix[0][0] + conf_matrix[1][1]) / np.sum(conf_matrix)\n",
        "    precision = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[0][1])\n",
        "    recall = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[1][0])\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    print(\"Accuracy: {}\".format(accuracy))\n",
        "    print(\"Precision: {}\".format(precision))\n",
        "    print(\"Recall: {}\".format(recall))\n",
        "    print(\"F1-Score: {}\\n\".format(f1_score))\n",
        "    print('='*89)\n",
        "    precs.append(0 if np.isnan(precision) else precision)\n",
        "    recs.append(0 if np.isnan(recall) else recall)\n",
        "    f1s.append(0 if np.isnan(f1_score) else f1_score)\n",
        "    # precs.append(precision)\n",
        "    # recs.append(recall)\n",
        "    # f1s.append(f1_score)\n",
        "print(np.mean(precs), np.mean(recs), np.mean(f1s))"
      ],
      "metadata": {
        "id": "pj2W9P-3iQgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text"
      ],
      "metadata": {
        "id": "O6QGWMhjmBPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `text_features_whole.py`\n",
        "Tries to extract textual features using \"ELMoForManyLangs\""
      ],
      "metadata": {
        "id": "mNgR526yOhXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- From the [GitHub repository](https://github.com/HIT-SCIR/ELMoForManyLangs?tab=readme-ov-file) download the model and place in the Google Drive.\n",
        "- If using the latest version of `overrides`, delete the decorator from `/content/ELMoForManyLangs/elmoformanylangs/modules/highway.py`.\n",
        "- Inside `ELMoForManyLangs/zhs.model`, set `\"config_path\"` to `\"/content/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\"`"
      ],
      "metadata": {
        "id": "8FJVQ0HyDoai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install overrides\n",
        "!git clone https://github.com/HIT-SCIR/ELMoForManyLangs.git\n",
        "!python ELMoForManyLangs/setup.py install\n",
        "!pip install overrides==4.1.2"
      ],
      "metadata": {
        "id": "A5WlpvVCoBPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wave\n",
        "import librosa\n",
        "import re\n",
        "from tqdm.notebook import trange, tqdm\n",
        "# from allennlp.commands.elmo import ElmoEmbedder\n",
        "import os\n",
        "from ELMoForManyLangs.elmoformanylangs import Embedder\n",
        "# import pkuseg\n",
        "# import thulac\n",
        "# from pyhanlp import HanLP\n",
        "import jieba\n",
        "# seg = pkuseg.pkuseg()\n",
        "# thu1 = thulac.thulac(seg_only=True)"
      ],
      "metadata": {
        "id": "FW2SbfnRCqKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elmo = Embedder(f'{BASELINE_DIR}/Model/ELMoForManyLangs/zhs.model')\n",
        "topics = ['positive', 'neutral', 'negative']\n",
        "answers = {}\n",
        "text_features = []\n",
        "text_targets = []"
      ],
      "metadata": {
        "id": "QTHnewvSDSbr",
        "outputId": "ecfb9b6d-07ac-41a6-f8f6-eaca231e1835",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:elmoformanylangs:char embedding size: 6169\n",
            "INFO:elmoformanylangs:word embedding size: 71222\n",
            "INFO:elmoformanylangs:Model(\n",
            "  (token_embedder): ConvTokenEmbedder(\n",
            "    (word_emb_layer): EmbeddingLayer(\n",
            "      (embedding): Embedding(71222, 100, padding_idx=3)\n",
            "    )\n",
            "    (char_emb_layer): EmbeddingLayer(\n",
            "      (embedding): Embedding(6169, 50, padding_idx=6166)\n",
            "    )\n",
            "    (convolutions): ModuleList(\n",
            "      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n",
            "      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n",
            "      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n",
            "      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n",
            "      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n",
            "      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n",
            "      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n",
            "    )\n",
            "    (highways): Highway(\n",
            "      (_layers): ModuleList(\n",
            "        (0-1): 2 x Linear(in_features=2048, out_features=4096, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (projection): Linear(in_features=2148, out_features=512, bias=True)\n",
            "  )\n",
            "  (encoder): ElmobiLm(\n",
            "    (forward_layer_0): LstmCellWithProjection(\n",
            "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
            "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
            "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
            "    )\n",
            "    (backward_layer_0): LstmCellWithProjection(\n",
            "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
            "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
            "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
            "    )\n",
            "    (forward_layer_1): LstmCellWithProjection(\n",
            "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
            "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
            "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
            "    )\n",
            "    (backward_layer_1): LstmCellWithProjection(\n",
            "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
            "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
            "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(text_features, text_targets, folder):\n",
        "    for index in trange(114):\n",
        "        if os.path.isdir(f'{DATASET_DIR}/{folder}{index+1}'):\n",
        "            answers[index+1] = []\n",
        "            for topic in topics:\n",
        "                with open(f'{DATASET_DIR}/{folder}{index+1}/{topic}.txt' ,'r') as f:\n",
        "                    lines = f.readlines()[0]\n",
        "                    # seg_text = seg.cut(lines)\n",
        "                    # seg_text = thu1.cut(lines)\n",
        "                    # seg_text_iter = HanLP.segment(lines)\n",
        "                    seg_text_iter = jieba.cut(lines, cut_all=False)\n",
        "                    answers[index+1].append([item for item in seg_text_iter])\n",
        "                    # answers[dir].append(seg_text)\n",
        "\n",
        "            with open(f'{DATASET_DIR}/{folder}{index+1}/new_label.txt') as fli:\n",
        "                target = float(fli.readline())\n",
        "            text_targets.append(1 if target >= 53 else 0)\n",
        "            # text_targets.append(target)\n",
        "            text_features.append([np.array(item).mean(axis=0) for item in elmo.sents2elmo(answers[index+1])])"
      ],
      "metadata": {
        "id": "ADuOnO8ImFEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_features(text_features, text_targets, 't_')\n",
        "extract_features(text_features, text_targets, 'v_')\n",
        "\n",
        "print(\"Saving npz file locally...\")\n",
        "np.savez(f'{BASELINE_DIR}/Features/TextWhole/whole_samples_clf_avg.npz', text_features)\n",
        "np.savez(f'{BASELINE_DIR}/Features/TextWhole/whole_labels_clf_avg.npz', text_targets)"
      ],
      "metadata": {
        "id": "jZVpznh6GGRB",
        "outputId": "be4d9c18-f63b-46c2-93c5-ad6e0bb94c72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "444ac9c047d348a7a743e719d34d5357",
            "69bacb88b3854f6e97ab039a3325b641",
            "23355b514bb5445ea7cc98db021cd983",
            "e8820441c56940cea37d2576066351e1",
            "8b4fea35e89243b4b3ce74e16375bb74",
            "7114c0e18b35411b9996742fa0507283",
            "a8cb1ce9b0ee4960a6a8df991c6e7a03",
            "efc451e52ea54afda02d3a43a454afe6",
            "46e0c7ae81164d808276f1c012bf9b72",
            "969a402499fa445ba4ee86e1b77beab5",
            "1957c7190f824f6b99b6ecd06678335c",
            "9166a6768d6646659927f127514db35f",
            "b5d19865740540edb94a1f0aa2565161",
            "803cf4f39e02434a9c47fb9b3dc88e94",
            "3f5c0ecc39854b1a9e59b9819a1a004f",
            "dfcb3c129bb4474795b5ff39854676d2",
            "4ec9a908a012446ab8b152bba9ccb7ce",
            "a1d540f250754fb2b68a7a583ddc5ac9",
            "88966dffcdb746dd9f9e60a6d547b9f9",
            "408fcd226c45469a944cb52b3ef73b70",
            "9d87ba61bdd146f2b8ef8f63e6a09101",
            "19b039bc15ed4981be05c2663d4a6afb"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/114 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "444ac9c047d348a7a743e719d34d5357"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.700 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.700 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n",
            "INFO:elmoformanylangs:1 batches, avg len: 70.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 19.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 33.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 12.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 32.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 109.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 62.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 45.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 150.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 37.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 87.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 21.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 61.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 25.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 39.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 13.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 60.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 28.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 15.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 42.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 25.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 53.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 18.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 26.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 23.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 42.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 14.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 60.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 38.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 28.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 75.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 80.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 60.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 229.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 27.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 69.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 19.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 13.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 113.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 44.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 19.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 86.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 13.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 75.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 15.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 24.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 21.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 81.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 21.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 78.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 11.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 31.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 25.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 20.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 46.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 15.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 24.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 59.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 97.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 23.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 61.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 57.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 11.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 33.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 34.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 20.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 19.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 51.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 38.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 15.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 23.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 31.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 32.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 33.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 40.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 22.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 24.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 69.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 74.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 11.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 10.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 31.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 197.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/114 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9166a6768d6646659927f127514db35f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:elmoformanylangs:1 batches, avg len: 28.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 17.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 27.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 13.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 84.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 171.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 68.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 25.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 26.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 35.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 58.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 20.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 39.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 28.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 48.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 22.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 14.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 128.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 9.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 55.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 25.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 34.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 12.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 48.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 34.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 22.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 32.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 40.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 9.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 9.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 18.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 48.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 25.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 46.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 51.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 22.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 22.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 7.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 36.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 9.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 81.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 40.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 31.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 71.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 86.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 21.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 12.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 14.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 108.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 22.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 62.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 11.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 77.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 113.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 21.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 7.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 53.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 14.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 80.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 9.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 15.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 24.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 26.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 26.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 38.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 36.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 11.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 17.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 18.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 24.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 27.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 49.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 52.7\n",
            "INFO:elmoformanylangs:1 batches, avg len: 18.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 17.3\n",
            "INFO:elmoformanylangs:1 batches, avg len: 16.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 10.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 16.0\n",
            "INFO:elmoformanylangs:1 batches, avg len: 21.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving npz file locally...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `text_bilstm_whole.py`\n",
        "Trains a BiLSTM on the textual features extracted from each of the 3 folds (using indices)"
      ],
      "metadata": {
        "id": "sREWigzzPHF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import itertools"
      ],
      "metadata": {
        "id": "o6EOF7ofQKn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_features = np.load(f'{BASELINE_DIR}/Features/TextWhole/whole_samples_clf_avg.npz')['arr_0']\n",
        "text_targets = np.load(f'{BASELINE_DIR}/Features/TextWhole/whole_labels_clf_avg.npz')['arr_0']\n",
        "text_dep_idxs_tmp = np.where(text_targets == 1)[0]\n",
        "text_non_idxs = np.where(text_targets == 0)[0]"
      ],
      "metadata": {
        "id": "hLP5F4gXQW23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextBiLSTM(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(TextBiLSTM, self).__init__()\n",
        "        self.num_classes = config['num_classes']\n",
        "        self.learning_rate = config['learning_rate']\n",
        "        self.dropout = config['dropout']\n",
        "        self.hidden_dims = config['hidden_dims']\n",
        "        self.rnn_layers = config['rnn_layers']\n",
        "        self.embedding_size = config['embedding_size']\n",
        "        self.bidirectional = config['bidirectional']\n",
        "\n",
        "        self.build_model()\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(net):\n",
        "        for name, param in net.named_parameters():\n",
        "            if 'ln' not in name:\n",
        "                if 'bias' in name:\n",
        "                    nn.init.constant_(param, 0.0)\n",
        "                elif 'weight' in name:\n",
        "                    nn.init.xavier_uniform_(param)\n",
        "\n",
        "    def build_model(self):\n",
        "        # attention layer\n",
        "        self.attention_layer = nn.Sequential(\n",
        "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # self.attention_weights = self.attention_weights.view(self.hidden_dims, 1)\n",
        "\n",
        "        # 双层lstm\n",
        "        self.lstm_net = nn.LSTM(self.embedding_size, self.hidden_dims,\n",
        "                                num_layers=self.rnn_layers, dropout=self.dropout,\n",
        "                                bidirectional=self.bidirectional)\n",
        "\n",
        "        # FC层\n",
        "        # self.fc_out = nn.Linear(self.hidden_dims, self.num_classes)\n",
        "        self.fc_out = nn.Sequential(\n",
        "            # nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.hidden_dims, self.num_classes),\n",
        "            # nn.ReLU(),\n",
        "            nn.Softmax(dim=1),\n",
        "        )\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(self.embedding_size)\n",
        "        self.ln2 = nn.LayerNorm(self.hidden_dims)\n",
        "\n",
        "\n",
        "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
        "        '''\n",
        "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
        "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
        "        :return: [batch_size, n_hidden]\n",
        "        '''\n",
        "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
        "        # h [batch_size, time_step, hidden_dims]\n",
        "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
        "        # h = lstm_out\n",
        "        # [batch_size, num_layers * num_directions, n_hidden]\n",
        "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
        "        # [batch_size, 1, n_hidden]\n",
        "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
        "        # atten_w [batch_size, 1, hidden_dims]\n",
        "        atten_w = self.attention_layer(lstm_hidden)\n",
        "        # m [batch_size, time_step, hidden_dims]\n",
        "        m = nn.Tanh()(h)\n",
        "        # atten_context [batch_size, 1, time_step]\n",
        "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
        "        # softmax_w [batch_size, 1, time_step]\n",
        "        softmax_w = F.softmax(atten_context, dim=-1)\n",
        "        # context [batch_size, 1, hidden_dims]\n",
        "        context = torch.bmm(softmax_w, h)\n",
        "        result = context.squeeze(1)\n",
        "        return result\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x : [len_seq, batch_size, embedding_dim]\n",
        "        x = x.permute(1, 0, 2)\n",
        "        # x = self.ln1(x)\n",
        "        output, (final_hidden_state, _) = self.lstm_net(x)\n",
        "        # output : [batch_size, len_seq, n_hidden * 2]\n",
        "        output = output.permute(1, 0, 2)\n",
        "        # final_hidden_state : [batch_size, num_layers * num_directions, n_hidden]\n",
        "        final_hidden_state = final_hidden_state.permute(1, 0, 2)\n",
        "        # final_hidden_state = torch.mean(final_hidden_state, dim=0, keepdim=True)\n",
        "        # atten_out = self.attention_net(output, final_hidden_state)\n",
        "        atten_out = self.attention_net_with_w(output, final_hidden_state)\n",
        "        # atten_out = self.ln2(atten_out)\n",
        "        return self.fc_out(atten_out)"
      ],
      "metadata": {
        "id": "HSFSZuBEP2_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save(model, filename):\n",
        "    save_filename = '{}.pt'.format(filename)\n",
        "    torch.save(model, save_filename)\n",
        "    print('Saved as %s' % save_filename)\n",
        "\n",
        "def standard_confusion_matrix(y_test, y_test_pred):\n",
        "    \"\"\"\n",
        "    Make confusion matrix with format:\n",
        "                  -----------\n",
        "                  | TP | FP |\n",
        "                  -----------\n",
        "                  | FN | TN |\n",
        "                  -----------\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : ndarray - 1D\n",
        "    y_pred : ndarray - 1D\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray - 2D\n",
        "    \"\"\"\n",
        "    [[tn, fp], [fn, tp]] = confusion_matrix(y_test, y_test_pred)\n",
        "    return np.array([[tp, fp], [fn, tn]])\n",
        "\n",
        "def model_performance(y_test, y_test_pred_proba):\n",
        "    \"\"\"\n",
        "    Evaluation metrics for network performance.\n",
        "    \"\"\"\n",
        "    y_test_pred = y_test_pred_proba.data.max(1, keepdim=True)[1]\n",
        "\n",
        "    # Computing confusion matrix for test dataset\n",
        "    conf_matrix = standard_confusion_matrix(y_test, y_test_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    return y_test_pred, conf_matrix\n",
        "\n",
        "def train(epoch, train_idxs):\n",
        "    global lr, train_acc\n",
        "    model.train()\n",
        "    batch_idx = 1\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    X_train = text_features[train_idxs]\n",
        "    Y_train = text_targets[train_idxs]\n",
        "    for i in range(0, X_train.shape[0], config['batch_size']):\n",
        "        if i + config['batch_size'] > X_train.shape[0]:\n",
        "            x, y = X_train[i:], Y_train[i:]\n",
        "        else:\n",
        "            x, y = X_train[i:(i + config['batch_size'])], Y_train[i:(\n",
        "                i + config['batch_size'])]\n",
        "        if config['cuda']:\n",
        "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True).cuda(), Variable(torch.from_numpy(y)).cuda()\n",
        "        else:\n",
        "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True), \\\n",
        "                Variable(torch.from_numpy(y))\n",
        "\n",
        "        # 将模型的参数梯度设置为0\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        #print(pred.shape, y.shape)\n",
        "        correct += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
        "        loss = criterion(output, y)\n",
        "        # 后向传播调整参数\n",
        "        loss.backward()\n",
        "        # 根据梯度更新网络参数\n",
        "        optimizer.step()\n",
        "        batch_idx += 1\n",
        "        # loss.item()能够得到张量中的元素值\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    train_acc = correct\n",
        "    print(\n",
        "        'Train Epoch: {:2d}\\t Learning rate: {:.4f}\\tLoss: {:.6f}\\t Accuracy: {}/{} ({:.0f}%)\\n '\n",
        "        .format(epoch + 1, config['learning_rate'], total_loss, correct,\n",
        "                X_train.shape[0], 100. * correct / X_train.shape[0]))\n",
        "\n",
        "\n",
        "def evaluate(model, test_idxs, fold, train_idxs):\n",
        "    model.eval()\n",
        "    batch_idx = 1\n",
        "    total_loss = 0\n",
        "    global max_f1, max_acc, min_mae, X_test_lens, max_prec, max_rec\n",
        "    pred = np.array([])\n",
        "    with torch.no_grad():\n",
        "        if config['cuda']:\n",
        "            x, y = Variable(torch.from_numpy(text_features[test_idxs]).type(torch.FloatTensor), requires_grad=True).cuda(),\\\n",
        "                Variable(torch.from_numpy(text_targets[test_idxs])).cuda()\n",
        "        else:\n",
        "            x, y = Variable(torch.from_numpy(text_features[test_idxs]).type(torch.FloatTensor), requires_grad=True), \\\n",
        "                Variable(torch.from_numpy(text_targets[test_idxs])).type(torch.LongTensor)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "        total_loss += loss.item()\n",
        "        y_test_pred, conf_matrix = model_performance(y, output.cpu())\n",
        "        accuracy = float(conf_matrix[0][0] + conf_matrix[1][1]) / np.sum(conf_matrix)\n",
        "        precision = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[0][1])\n",
        "        recall = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[1][0])\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "        print(\"Accuracy: {}\".format(accuracy))\n",
        "        print(\"Precision: {}\".format(precision))\n",
        "        print(\"Recall: {}\".format(recall))\n",
        "        print(\"F1-Score: {}\\n\".format(f1_score))\n",
        "        print('=' * 89)\n",
        "\n",
        "        if max_f1 <= f1_score and train_acc > len(train_idxs)*0.9 and f1_score > 0.5:\n",
        "            max_f1 = f1_score\n",
        "            max_acc = accuracy\n",
        "            max_rec = recall\n",
        "            max_prec = precision\n",
        "            save(model, f\"{BASELINE_DIR}/Model/ClassificationWhole/Text/BiLSTM_{config['hidden_dims']}_{max_f1:.2f}_{fold}\")\n",
        "            print('*' * 64)\n",
        "            print('model saved: f1: {}\\tacc: {}'.format(max_f1, max_acc))\n",
        "            print('*' * 64)\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "def get_param_group(model):\n",
        "    nd_list = []\n",
        "    param_list = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'ln' in name:\n",
        "            nd_list.append(param)\n",
        "        else:\n",
        "            param_list.append(param)\n",
        "    return [{'params': param_list, 'weight_decay': 1e-5}, {'params': nd_list, 'weight_decay': 0}]"
      ],
      "metadata": {
        "id": "7X9vago7QdvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'num_classes': 2,\n",
        "    'dropout': 0.5,\n",
        "    'rnn_layers': 2,\n",
        "    'embedding_size': 1024,\n",
        "    'batch_size': 4,\n",
        "    'epochs': 150,\n",
        "    'learning_rate': 1e-5,\n",
        "    'hidden_dims': 128,\n",
        "    'bidirectional': True,\n",
        "    'cuda': False,\n",
        "}"
      ],
      "metadata": {
        "id": "9MFXfTnbQpln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_idxs_tmps = [\n",
        "    np.load(f'{BASELINE_DIR}/Features/TrainIdx/train_idxs_0.63_1.npy', allow_pickle=True),\n",
        "    np.load(f'{BASELINE_DIR}/Features/TrainIdx/train_idxs_0.60_2.npy', allow_pickle=True),\n",
        "    np.load(f'{BASELINE_DIR}/Features/TrainIdx/train_idxs_0.60_3.npy', allow_pickle=True)\n",
        "]\n",
        "fold = 1\n",
        "\n",
        "for idx_idx, train_idxs_tmp in enumerate(train_idxs_tmps):\n",
        "    # if idx_idx != 2:\n",
        "    #     continue\n",
        "    test_idxs_tmp = list(set(list(text_dep_idxs_tmp)+list(text_non_idxs)) - set(train_idxs_tmp))\n",
        "    train_idxs, test_idxs = [], []\n",
        "    # depression data augmentation\n",
        "    for idx in train_idxs_tmp:\n",
        "        if idx in text_dep_idxs_tmp:\n",
        "            feat = text_features[idx]\n",
        "            count = 0\n",
        "            resample_idxs = [0,1,2,3,4,5]\n",
        "            for i in itertools.permutations(feat, feat.shape[0]):\n",
        "                if count in resample_idxs:\n",
        "                    text_features = np.vstack((text_features, np.expand_dims(list(i), 0)))\n",
        "                    text_targets = np.hstack((text_targets, 1))\n",
        "                    train_idxs.append(len(text_features)-1)\n",
        "                count += 1\n",
        "        else:\n",
        "            train_idxs.append(idx)\n",
        "\n",
        "    for idx in test_idxs_tmp:\n",
        "        if idx in text_dep_idxs_tmp:\n",
        "            feat = text_features[idx]\n",
        "            count = 0\n",
        "            # resample_idxs = random.sample(range(6), 4)\n",
        "            resample_idxs = [0,1,4,5]\n",
        "            for i in itertools.permutations(feat, feat.shape[0]):\n",
        "                if count in resample_idxs:\n",
        "                    text_features = np.vstack((text_features, np.expand_dims(list(i), 0)))\n",
        "                    text_targets = np.hstack((text_targets, 1))\n",
        "                    test_idxs.append(len(text_features)-1)\n",
        "                count += 1\n",
        "        else:\n",
        "            test_idxs.append(idx)\n",
        "\n",
        "    model = TextBiLSTM(config)\n",
        "\n",
        "    param_group = get_param_group(model)\n",
        "    optimizer = optim.AdamW(param_group, lr=config['learning_rate'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    max_f1 = -1\n",
        "    max_acc = -1\n",
        "    max_rec = -1\n",
        "    max_prec = -1\n",
        "    train_acc = -1\n",
        "\n",
        "    for ep in range(1, config['epochs']):\n",
        "        train(ep, train_idxs)\n",
        "        tloss = evaluate(model, test_idxs, fold, train_idxs)\n",
        "    fold += 1"
      ],
      "metadata": {
        "id": "_UQEXMgOQbX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `TextModelChecking.py`\n",
        "Just evaluates some \"BiLSTM_128_0\" models on each of the 3 folds"
      ],
      "metadata": {
        "id": "inM0bpczYYLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wave\n",
        "import re\n",
        "import os\n",
        "import tensorflow.compat.v1 as tf\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle"
      ],
      "metadata": {
        "id": "C4-2I54rYoi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prefix = os.path.abspath(os.path.join(os.getcwd(), \".\"))\n",
        "# text_features = np.load(os.path.join(prefix, 'Features/Text/whole_samples_clf_avg.npz'))['arr_0']\n",
        "# text_targets = np.load(os.path.join(prefix, 'Features/Text/whole_labels_clf_avg.npz'))['arr_0']\n",
        "\n",
        "# audio_dep_idxs = np.where(text_targets == 1)[0]\n",
        "# audio_non_idxs = np.where(text_targets == 0)[0]\n",
        "# # train_dep_idxs_tmp = np.load(os.path.join(prefix, 'Features/Text/train_dep_idxs_0.80.npy'), allow_pickle=True)\n",
        "# # train_non_idxs = list(np.load(os.path.join(prefix, 'Features/Text/train_non_idxs_0.80.npy'), allow_pickle=True))\n",
        "# # train_dep_idxs_tmp = np.load(os.path.join(prefix, 'Features/Text/train_dep_idxs_0.65_2.npy'), allow_pickle=True)\n",
        "# # train_non_idxs = list(np.load(os.path.join(prefix, 'Features/Text/train_non_idxs_0.65_2.npy'), allow_pickle=True))\n",
        "# train_dep_idxs_tmp = np.load(os.path.join(prefix, 'Features/Text/train_dep_idxs_0.89_3.npy'), allow_pickle=True)\n",
        "# train_non_idxs = list(np.load(os.path.join(prefix, 'Features/Text/train_non_idxs_0.89_3.npy'), allow_pickle=True))\n",
        "\n",
        "# test_dep_idxs_tmp = list(set(audio_dep_idxs) - set(train_dep_idxs_tmp))\n",
        "# test_non_idxs = list(set(audio_non_idxs) - set(train_non_idxs))\n",
        "\n",
        "text_features = np.load(f'{BASELINE_DIR}/Features/TextWhole/whole_samples_clf_avg.npz')['arr_0']\n",
        "text_targets = np.load(f'{BASELINE_DIR}/Features/TextWhole/whole_labels_clf_avg.npz')['arr_0']\n",
        "text_dep_idxs_tmp = np.where(text_targets == 1)[0]\n",
        "text_non_idxs = np.where(text_targets == 0)[0]"
      ],
      "metadata": {
        "id": "dxva-cz4a3ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # training data augmentation\n",
        "# train_dep_idxs = []\n",
        "# for idx in train_dep_idxs_tmp:\n",
        "#     feat = text_features[idx]\n",
        "#     for i in itertools.permutations(feat, feat.shape[0]):\n",
        "#         text_features = np.vstack((text_features, np.expand_dims(list(i), 0)))\n",
        "#         text_targets = np.hstack((text_targets, 1))\n",
        "#         train_dep_idxs.append(len(text_features)-1)\n",
        "\n",
        "#         text_features = np.vstack((text_features, np.expand_dims(list(i), 0)))\n",
        "#         text_targets = np.hstack((text_targets, 1))\n",
        "#         train_dep_idxs.append(len(text_features)-1)\n",
        "\n",
        "# # test data augmentation\n",
        "# test_dep_idxs = []\n",
        "# for idx in test_dep_idxs_tmp:\n",
        "#     feat = text_features[idx]\n",
        "#     for i in itertools.permutations(feat, feat.shape[0]):\n",
        "#         text_features = np.vstack((text_features, np.expand_dims(list(i), 0)))\n",
        "#         text_targets = np.hstack((text_targets, 1))\n",
        "#         test_dep_idxs.append(len(text_features)-1)"
      ],
      "metadata": {
        "id": "jFm-YROQbAnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standard_confusion_matrix(y_test, y_test_pred):\n",
        "    \"\"\"\n",
        "    Make confusion matrix with format:\n",
        "                  -----------\n",
        "                  | TP | FP |\n",
        "                  -----------\n",
        "                  | FN | TN |\n",
        "                  -----------\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : ndarray - 1D\n",
        "    y_pred : ndarray - 1D\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray - 2D\n",
        "    \"\"\"\n",
        "    [[tn, fp], [fn, tp]] = confusion_matrix(y_test, y_test_pred)\n",
        "    return np.array([[tp, fp], [fn, tn]])\n",
        "\n",
        "\n",
        "def model_performance(y_test, y_test_pred_proba):\n",
        "    \"\"\"\n",
        "    Evaluation metrics for network performance.\n",
        "    \"\"\"\n",
        "    # y_test_pred = y_test_pred_proba.data.max(1, keepdim=True)[1]\n",
        "    y_test_pred = y_test_pred_proba\n",
        "\n",
        "    # Computing confusion matrix for test dataset\n",
        "    conf_matrix = standard_confusion_matrix(y_test, y_test_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    return y_test_pred, conf_matrix"
      ],
      "metadata": {
        "id": "AjNe23o_bERs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextBiLSTM(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(TextBiLSTM, self).__init__()\n",
        "        self.num_classes = config['num_classes']\n",
        "        self.learning_rate = config['learning_rate']\n",
        "        self.dropout = config['dropout']\n",
        "        self.hidden_dims = config['hidden_dims']\n",
        "        self.rnn_layers = config['rnn_layers']\n",
        "        self.embedding_size = config['embedding_size']\n",
        "        self.bidirectional = config['bidirectional']\n",
        "\n",
        "        self.build_model()\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(net):\n",
        "        for name, param in net.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                nn.init.constant_(param, 0.0)\n",
        "            elif 'weight' in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "\n",
        "    def build_model(self):\n",
        "        # attention layer\n",
        "        self.attention_layer = nn.Sequential(\n",
        "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # self.attention_weights = self.attention_weights.view(self.hidden_dims, 1)\n",
        "\n",
        "        # 双层lstm\n",
        "        self.lstm_net = nn.LSTM(self.embedding_size, self.hidden_dims,\n",
        "                                num_layers=self.rnn_layers, dropout=self.dropout,\n",
        "                                bidirectional=self.bidirectional)\n",
        "\n",
        "        # self.init_weight()\n",
        "\n",
        "        # FC层\n",
        "        # self.fc_out = nn.Linear(self.hidden_dims, self.num_classes)\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.hidden_dims, self.num_classes),\n",
        "            # nn.ReLU(),\n",
        "            nn.Softmax(dim=1),\n",
        "        )\n",
        "\n",
        "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
        "        '''\n",
        "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
        "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
        "        :return: [batch_size, n_hidden]\n",
        "        '''\n",
        "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
        "        # h [batch_size, time_step, hidden_dims]\n",
        "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
        "        # h = lstm_out\n",
        "        # [batch_size, num_layers * num_directions, n_hidden]\n",
        "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
        "        # [batch_size, 1, n_hidden]\n",
        "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
        "        # atten_w [batch_size, 1, hidden_dims]\n",
        "        atten_w = self.attention_layer(lstm_hidden)\n",
        "        # m [batch_size, time_step, hidden_dims]\n",
        "        m = nn.Tanh()(h)\n",
        "        # atten_context [batch_size, 1, time_step]\n",
        "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
        "        # softmax_w [batch_size, 1, time_step]\n",
        "        softmax_w = F.softmax(atten_context, dim=-1)\n",
        "        # context [batch_size, 1, hidden_dims]\n",
        "        context = torch.bmm(softmax_w, h)\n",
        "        result = context.squeeze(1)\n",
        "        return result\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # x : [len_seq, batch_size, embedding_dim]\n",
        "        x = x.permute(1, 0, 2)\n",
        "        output, (final_hidden_state, final_cell_state) = self.lstm_net(x)\n",
        "        # output : [batch_size, len_seq, n_hidden * 2]\n",
        "        output = output.permute(1, 0, 2)\n",
        "        # final_hidden_state : [batch_size, num_layers * num_directions, n_hidden]\n",
        "        final_hidden_state = final_hidden_state.permute(1, 0, 2)\n",
        "        # final_hidden_state = torch.mean(final_hidden_state, dim=0, keepdim=True)\n",
        "        # atten_out = self.attention_net(output, final_hidden_state)\n",
        "        atten_out = self.attention_net_with_w(output, final_hidden_state)\n",
        "        return self.fc_out(atten_out)\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, rnn_layers, dropout, num_classes, text_hidden_dims, text_embed_size):\n",
        "        super(BiLSTM, self).__init__()\n",
        "\n",
        "        self.text_embed_size = text_embed_size\n",
        "        self.text_hidden_dims = text_hidden_dims\n",
        "        self.rnn_layers = rnn_layers\n",
        "        self.dropout = dropout\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # attention layer\n",
        "        self.attention_layer = nn.Sequential(\n",
        "            nn.Linear(self.text_hidden_dims, self.text_hidden_dims),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # 双层lstm\n",
        "        self.lstm_net = nn.LSTM(self.text_embed_size, self.text_hidden_dims,\n",
        "                                num_layers=self.rnn_layers, dropout=self.dropout,\n",
        "                                bidirectional=True)\n",
        "        # FC层\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.text_hidden_dims, self.text_hidden_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.text_hidden_dims, self.num_classes),\n",
        "            # nn.ReLU(),\n",
        "            nn.Softmax(dim=1),\n",
        "        )\n",
        "\n",
        "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
        "        '''\n",
        "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
        "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
        "        :return: [batch_size, n_hidden]\n",
        "        '''\n",
        "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
        "        # h [batch_size, time_step, hidden_dims]\n",
        "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
        "        # [batch_size, num_layers * num_directions, n_hidden]\n",
        "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
        "        # [batch_size, 1, n_hidden]\n",
        "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
        "        # atten_w [batch_size, 1, hidden_dims]\n",
        "        atten_w = self.attention_layer(lstm_hidden)\n",
        "        # m [batch_size, time_step, hidden_dims]\n",
        "        m = nn.Tanh()(h)\n",
        "        # atten_context [batch_size, 1, time_step]\n",
        "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
        "        # softmax_w [batch_size, 1, time_step]\n",
        "        softmax_w = F.softmax(atten_context, dim=-1)\n",
        "        # context [batch_size, 1, hidden_dims]\n",
        "        context = torch.bmm(softmax_w, h)\n",
        "        result = context.squeeze(1)\n",
        "        return result\n",
        "\n",
        "    def forward(self, x_text):\n",
        "        # x : [len_seq, batch_size, embedding_dim]\n",
        "        x_text = x_text.permute(1, 0, 2)\n",
        "        output, (final_hidden_state, _) = self.lstm_net(x_text)\n",
        "        # output : [batch_size, len_seq, n_hidden * 2]\n",
        "        output = output.permute(1, 0, 2)\n",
        "        # final_hidden_state : [batch_size, num_layers * num_directions, n_hidden]\n",
        "        final_hidden_state = final_hidden_state.permute(1, 0, 2)\n",
        "        # final_hidden_state = torch.mean(final_hidden_state, dim=0, keepdim=True)\n",
        "        # atten_out = self.attention_net(output, final_hidden_state)\n",
        "        atten_out = self.attention_net_with_w(output, final_hidden_state)\n",
        "        text_feature = self.fc_out(atten_out)\n",
        "\n",
        "        return text_feature"
      ],
      "metadata": {
        "id": "2uxKQ5hJbKrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_idxs):\n",
        "    model.eval()\n",
        "    batch_idx = 1\n",
        "    total_loss = 0\n",
        "    pred = torch.empty(config['batch_size'], 1).type(torch.LongTensor)\n",
        "    # X_test = text_features[test_dep_idxs+test_non_idxs]\n",
        "    # Y_test = text_targets[test_dep_idxs+test_non_idxs]\n",
        "    X_test = text_features[test_idxs]\n",
        "    Y_test = text_targets[test_idxs]\n",
        "    global max_train_acc, max_acc, max_f1\n",
        "    for i in range(0, X_test.shape[0], config['batch_size']):\n",
        "        if i + config['batch_size'] > X_test.shape[0]:\n",
        "            x, y = X_test[i:], Y_test[i:]\n",
        "        else:\n",
        "            x, y = X_test[i:(i+config['batch_size'])\n",
        "                          ], Y_test[i:(i+config['batch_size'])]\n",
        "        if config['cuda']:\n",
        "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True).cuda(\n",
        "            ),             Variable(torch.from_numpy(y)).cuda()\n",
        "        else:\n",
        "            x, y = Variable(torch.from_numpy(x).type(\n",
        "                torch.FloatTensor), requires_grad=True), Variable(torch.from_numpy(y))\n",
        "        with torch.no_grad():\n",
        "            output = model(x.squeeze(2))\n",
        "        pred = torch.cat((pred, output.data.max(1, keepdim=True)[1]))\n",
        "\n",
        "    y_test_pred, conf_matrix = model_performance(\n",
        "        Y_test, pred[config['batch_size']:])\n",
        "    print('Calculating additional test metrics...')\n",
        "    accuracy = float(conf_matrix[0][0] +\n",
        "                     conf_matrix[1][1]) / np.sum(conf_matrix)\n",
        "    precision = float(conf_matrix[0][0]) / \\\n",
        "        (conf_matrix[0][0] + conf_matrix[0][1])\n",
        "    recall = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[1][0])\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    print(\"Accuracy: {}\".format(accuracy))\n",
        "    print(\"Precision: {}\".format(precision))\n",
        "    print(\"Recall: {}\".format(recall))\n",
        "    print(\"F1-Score: {}\\n\".format(f1_score))\n",
        "    print('='*89)\n",
        "    return precision, recall, f1_score"
      ],
      "metadata": {
        "id": "msHHPK22bolF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_model_paths = ['BiLSTM_128_0.64_1.pt', 'BiLSTM_128_0.66_2.pt', 'BiLSTM_128_0.66_3.pt']\n",
        "train_idxs_tmps = [\n",
        "    np.load(f'{BASELINE_DIR}/Features/TrainIdx/train_idxs_0.63_1.npy', allow_pickle=True),\n",
        "    np.load(f'{BASELINE_DIR}/Features/TrainIdx/train_idxs_0.60_2.npy', allow_pickle=True),\n",
        "    np.load(f'{BASELINE_DIR}/Features/TrainIdx/train_idxs_0.60_3.npy', allow_pickle=True)\n",
        "]\n",
        "resample_idxs = [0, 1, 2, 3, 4, 5]\n",
        "fold = 1\n",
        "ps, rs, fs = [], [], []\n",
        "for idx_i, train_idxs_tmp in enumerate(train_idxs_tmps):\n",
        "    test_idxs_tmp = list(\n",
        "        set(list(text_dep_idxs_tmp)+list(text_non_idxs)) - set(train_idxs_tmp))\n",
        "    train_idxs, test_idxs = [], []\n",
        "    # depression data augmentation\n",
        "    for idx in train_idxs_tmp:\n",
        "        if idx in text_dep_idxs_tmp:\n",
        "            feat = text_features[idx]\n",
        "            count = 0\n",
        "            for i in itertools.permutations(feat, feat.shape[0]):\n",
        "                if count in resample_idxs:\n",
        "                    text_features = np.vstack(\n",
        "                        (text_features, np.expand_dims(list(i), 0)))\n",
        "                    text_targets = np.hstack((text_targets, 1))\n",
        "                    train_idxs.append(len(text_features)-1)\n",
        "                count += 1\n",
        "        else:\n",
        "            train_idxs.append(idx)\n",
        "\n",
        "    for idx in test_idxs_tmp:\n",
        "        if idx in text_dep_idxs_tmp:\n",
        "            feat = text_features[idx]\n",
        "            count = 0\n",
        "            # resample_idxs = random.sample(range(6), 4)\n",
        "            resample_idxs = [0,1,4,5]\n",
        "            for i in itertools.permutations(feat, feat.shape[0]):\n",
        "                if count in resample_idxs:\n",
        "                    text_features = np.vstack(\n",
        "                        (text_features, np.expand_dims(list(i), 0)))\n",
        "                    text_targets = np.hstack((text_targets, 1))\n",
        "                    test_idxs.append(len(text_features)-1)\n",
        "                count += 1\n",
        "        else:\n",
        "            test_idxs.append(idx)\n",
        "\n",
        "    config = {\n",
        "        'num_classes': 2,\n",
        "        'dropout': 0.5,\n",
        "        'rnn_layers': 2,\n",
        "        'embedding_size': 1024,\n",
        "        'batch_size': 4,\n",
        "        'epochs': 100,\n",
        "        'learning_rate': 2e-5,\n",
        "        'hidden_dims': 128,\n",
        "        'bidirectional': True,\n",
        "        'cuda': False,\n",
        "    }\n",
        "\n",
        "    text_lstm_model = torch.load(os.path.join(\n",
        "        prefix, 'Model/ClassificationWhole/Text/{}'.format(text_model_paths[idx_i])))\n",
        "\n",
        "    model = BiLSTM(config['rnn_layers'], config['dropout'], config['num_classes'],\n",
        "                   config['hidden_dims'], config['embedding_size'])\n",
        "\n",
        "    # model_state_dict = {}\n",
        "    # model_state_dict['lstm_net_audio.weight_ih_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_ih_l0']\n",
        "    # model_state_dict['lstm_net_audio.weight_hh_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_hh_l0']\n",
        "    # model_state_dict['lstm_net_audio.bias_ih_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_ih_l0']\n",
        "    # model_state_dict['lstm_net_audio.bias_hh_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_hh_l0']\n",
        "\n",
        "    # model_state_dict['lstm_net_audio.weight_ih_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_ih_l1']\n",
        "    # model_state_dict['lstm_net_audio.weight_hh_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_hh_l1']\n",
        "    # model_state_dict['lstm_net_audio.bias_ih_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_ih_l1']\n",
        "    # model_state_dict['lstm_net_audio.bias_hh_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_hh_l1']\n",
        "\n",
        "    # model_state_dict['fc_audio.1.weight'] = audio_lstm_model.state_dict()['fc_audio.1.weight']\n",
        "    # model_state_dict['fc_audio.1.bias'] = audio_lstm_model.state_dict()['fc_audio.1.bias']\n",
        "    # model_state_dict['fc_audio.4.weight'] = audio_lstm_model.state_dict()['fc_audio.4.weight']\n",
        "    # model_state_dict['fc_audio.4.bias'] = audio_lstm_model.state_dict()['fc_audio.4.bias']\n",
        "    # model_state_dict = text_lstm_model.state_dict()\n",
        "    # model.load_state_dict(model_state_dict)\n",
        "\n",
        "    # evaluate(text_features_test, fuse_targets_test, audio_lstm_model)\n",
        "    # evaluate(model, test_idxs)\n",
        "\n",
        "    p, r, f = evaluate(text_lstm_model, test_idxs)\n",
        "    ps.append(p)\n",
        "    rs.append(r)\n",
        "    fs.append(f)\n",
        "print('precison: {} \\n recall: {} \\n f1 score: {}'.format(np.mean(ps), np.mean(rs), np.mean(fs)))"
      ],
      "metadata": {
        "id": "7IZUvRj6YcLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `TextTraditionalClassifiers.py`\n",
        "Tries \"DecisionTreeClassifier\", \"LogisticRegression\", \"SVC\", and \"RandomForestClassifier\" on the same folds to compare against the proposed BiLSTM"
      ],
      "metadata": {
        "id": "zDvmTNjRcHVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "W4GT7UE5dIs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_features = np.load(f'{BASELINE_DIR}/Features/TextWhole/whole_samples_clf_avg.npz')['arr_0']\n",
        "text_targets = np.load(f'{BASELINE_DIR}/Features/TextWhole/whole_labels_clf_avg.npz')['arr_0']\n",
        "\n",
        "text_dep_idxs_tmp = np.where(text_targets == 1)[0]\n",
        "text_non_idxs = np.where(text_targets == 0)[0]"
      ],
      "metadata": {
        "id": "HsgIqPRMdXUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_performance(y_test, y_test_pred_proba):\n",
        "    \"\"\"\n",
        "    Evaluation metrics for network performance.\n",
        "    \"\"\"\n",
        "#     y_test_pred = y_test_pred_proba.data.max(1, keepdim=True)[1]\n",
        "    y_test_pred = y_test_pred_proba\n",
        "\n",
        "    # Computing confusion matrix for test dataset\n",
        "    conf_matrix = standard_confusion_matrix(y_test, y_test_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    return y_test_pred, conf_matrix\n",
        "\n",
        "def standard_confusion_matrix(y_test, y_test_pred):\n",
        "    [[tn, fp], [fn, tp]] = confusion_matrix(y_test, y_test_pred)\n",
        "    return np.array([[tp, fp], [fn, tn]])"
      ],
      "metadata": {
        "id": "3EscJ7XndkzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_idxs_tmps = [\n",
        "    np.load(f'{BASELINE_DIR}/Features/TrainIdx/train_idxs_0.63_1.npy', allow_pickle=True),\n",
        "    np.load(f'{BASELINE_DIR}/Features/TrainIdx/train_idxs_0.65_2.npy', allow_pickle=True),\n",
        "    np.load(f'{BASELINE_DIR}/Features/TrainIdx/train_idxs_0.60_3.npy', allow_pickle=True)\n",
        "]\n",
        "precs, recs, f1s = [], [], []\n",
        "\n",
        "for idx_idx, train_idxs_tmp in enumerate(train_idxs_tmps):\n",
        "    test_idxs_tmp = list(set(list(text_dep_idxs_tmp)+list(text_non_idxs)) - set(train_idxs_tmp))\n",
        "    train_idxs, test_idxs = [], []\n",
        "\n",
        "    # depression data augmentation\n",
        "    for idx in train_idxs_tmp:\n",
        "        if idx in text_dep_idxs_tmp:\n",
        "            feat = text_features[idx]\n",
        "            count = 0\n",
        "            resample_idxs = [0,1,2,3,4,5]\n",
        "            for i in itertools.permutations(feat, feat.shape[0]):\n",
        "                if count in resample_idxs:\n",
        "                    text_features = np.vstack((text_features, np.expand_dims(list(i), 0)))\n",
        "                    text_targets = np.hstack((text_targets, 1))\n",
        "                    train_idxs.append(len(text_features)-1)\n",
        "                count += 1\n",
        "        else:\n",
        "            train_idxs.append(idx)\n",
        "\n",
        "    for idx in test_idxs_tmp:\n",
        "        if idx in text_dep_idxs_tmp:\n",
        "            feat = text_features[idx]\n",
        "            count = 0\n",
        "            # resample_idxs = random.sample(range(6), 4)\n",
        "            resample_idxs = [0,1,4,5]\n",
        "            for i in itertools.permutations(feat, feat.shape[0]):\n",
        "                if count in resample_idxs:\n",
        "                    text_features = np.vstack((text_features, np.expand_dims(list(i), 0)))\n",
        "                    text_targets = np.hstack((text_targets, 1))\n",
        "                    test_idxs.append(len(text_features)-1)\n",
        "                count += 1\n",
        "        else:\n",
        "            test_idxs.append(idx)\n",
        "    # train_idxs = train_idxs_tmp\n",
        "    # test_idxs = test_idxs_tmp\n",
        "\n",
        "    X_train = text_features[train_idxs]\n",
        "    Y_train = text_targets[train_idxs]\n",
        "    X_test = text_features[test_idxs]\n",
        "    Y_test = text_targets[test_idxs]\n",
        "\n",
        "    # Decision Tree\n",
        "    from sklearn import tree\n",
        "    clf = tree.DecisionTreeClassifier(max_depth=20)\n",
        "\n",
        "    # svm\n",
        "    # from sklearn.svm import SVC\n",
        "    # clf = SVC(kernel='rbf', gamma='auto')\n",
        "\n",
        "    # rf\n",
        "    # from sklearn.ensemble import RandomForestClassifier\n",
        "    # clf = RandomForestClassifier(n_estimators=10, max_depth=20)\n",
        "\n",
        "    # lr\n",
        "    # from sklearn.linear_model import LogisticRegression\n",
        "    # clf = LogisticRegression()\n",
        "\n",
        "    clf.fit([f.flatten() for f in X_train], Y_train)\n",
        "    pred = clf.predict([f.flatten() for f in X_test])\n",
        "    # clf.fit([f.sum(axis=0) for f in X_train], Y_train)\n",
        "    # pred = clf.predict([f.sum(axis=0) for f in X_test])\n",
        "\n",
        "    y_test_pred, conf_matrix = model_performance(Y_test, pred)\n",
        "\n",
        "    # custom evaluation metrics\n",
        "    print('Calculating additional test metrics...')\n",
        "    accuracy = float(conf_matrix[0][0] + conf_matrix[1][1]) / np.sum(conf_matrix)\n",
        "    precision = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[0][1])\n",
        "    recall = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[1][0])\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    print(\"Accuracy: {}\".format(accuracy))\n",
        "    print(\"Precision: {}\".format(precision))\n",
        "    print(\"Recall: {}\".format(recall))\n",
        "    print(\"F1-Score: {}\\n\".format(f1_score))\n",
        "    print('='*89)\n",
        "    # precs.append(0 if np.isnan(precision) else precision)\n",
        "    # recs.append(0 if np.isnan(recall) else recall)\n",
        "    # f1s.append(0 if np.isnan(f1_score) else f1_score)\n",
        "    precs.append(precision)\n",
        "    recs.append(recall)\n",
        "    f1s.append(f1_score)\n",
        "print(np.mean(precs), np.mean(recs), np.mean(f1s))"
      ],
      "metadata": {
        "id": "zA1YvKxMcbV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fusion"
      ],
      "metadata": {
        "id": "_JIA-CrKfK6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `fuse_net_whole.py`\n",
        "Connects altogether using 3-fold indices and textual \"BiLSTM\" features along with \"BiLSTM-GRU-VLAD\" features and their according saved models"
      ],
      "metadata": {
        "id": "7aKb6nCbfQ2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install python_speech_features"
      ],
      "metadata": {
        "id": "nO18pM6fhXo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wave\n",
        "import librosa\n",
        "from python_speech_features import *\n",
        "import re\n",
        "# from allennlp.commands.elmo import ElmoEmbedder\n",
        "import os\n",
        "import tensorflow.compat.v1 as tf\n",
        "import itertools"
      ],
      "metadata": {
        "id": "Pfe_2-AXhE-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text_features = np.load(f'{BASELINE_DIR}/Features/TextWhole/whole_samples_clf_avg.npz')['arr_0']\n",
        "text_targets = np.load(f'{BASELINE_DIR}/Features/TextWhole/whole_labels_clf_avg.npz')['arr_0']\n",
        "\n",
        "audio_features = np.squeeze(np.load(f'{BASELINE_DIR}/Features/AudioWhole/whole_samples_clf_256.npz')['arr_0'], axis=2)\n",
        "audio_targets = np.load(f'{BASELINE_DIR}/Features/AudioWhole/whole_labels_clf_256.npz')['arr_0']\n",
        "\n",
        "fuse_features = [[audio_features[i], text_features[i]] for i in range(text_features.shape[0])]\n",
        "fuse_targets = text_targets\n",
        "\n",
        "fuse_dep_idxs = np.where(text_targets == 1)[0]\n",
        "fuse_non_idxs = np.where(text_targets == 0)[0]"
      ],
      "metadata": {
        "id": "tM-kE8nWhdI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save(model, filename):\n",
        "    save_filename = '{}.pt'.format(filename)\n",
        "    torch.save(model, save_filename)\n",
        "    print('Saved as %s' % save_filename)\n",
        "\n",
        "def standard_confusion_matrix(y_test, y_test_pred):\n",
        "    \"\"\"\n",
        "    Make confusion matrix with format:\n",
        "                  -----------\n",
        "                  | TP | FP |\n",
        "                  -----------\n",
        "                  | FN | TN |\n",
        "                  -----------\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : ndarray - 1D\n",
        "    y_pred : ndarray - 1D\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray - 2D\n",
        "    \"\"\"\n",
        "    [[tn, fp], [fn, tp]] = confusion_matrix(y_test, y_test_pred)\n",
        "    return np.array([[tp, fp], [fn, tn]])\n",
        "\n",
        "def model_performance(y_test, y_test_pred_proba):\n",
        "    \"\"\"\n",
        "    Evaluation metrics for network performance.\n",
        "    \"\"\"\n",
        "    # y_test_pred = y_test_pred_proba.data.max(1, keepdim=True)[1]\n",
        "    y_test_pred = y_test_pred_proba\n",
        "\n",
        "    # Computing confusion matrix for test dataset\n",
        "    conf_matrix = standard_confusion_matrix(y_test, y_test_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    return y_test_pred, conf_matrix"
      ],
      "metadata": {
        "id": "IlAX1CUGiDs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextBiLSTM(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(TextBiLSTM, self).__init__()\n",
        "        self.num_classes = config['num_classes']\n",
        "        self.learning_rate = config['learning_rate']\n",
        "        self.dropout = config['dropout']\n",
        "        self.hidden_dims = config['hidden_dims']\n",
        "        self.rnn_layers = config['rnn_layers']\n",
        "        self.embedding_size = config['embedding_size']\n",
        "        self.bidirectional = config['bidirectional']\n",
        "\n",
        "        self.build_model()\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(net):\n",
        "        for name, param in net.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                nn.init.constant_(param, 0.0)\n",
        "            elif 'weight' in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "\n",
        "    def build_model(self):\n",
        "        # attention layer\n",
        "        self.attention_layer = nn.Sequential(\n",
        "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # self.attention_weights = self.attention_weights.view(self.hidden_dims, 1)\n",
        "\n",
        "        # 双层lstm\n",
        "        self.lstm_net = nn.LSTM(self.embedding_size, self.hidden_dims,\n",
        "                                num_layers=self.rnn_layers, dropout=self.dropout,\n",
        "                                bidirectional=self.bidirectional)\n",
        "\n",
        "        # self.init_weight()\n",
        "\n",
        "        # FC层\n",
        "        # self.fc_out = nn.Linear(self.hidden_dims, self.num_classes)\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.hidden_dims, self.num_classes),\n",
        "            # nn.ReLU(),\n",
        "            nn.Softmax(dim=1),\n",
        "        )\n",
        "\n",
        "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
        "        '''\n",
        "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
        "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
        "        :return: [batch_size, n_hidden]\n",
        "        '''\n",
        "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
        "        # h [batch_size, time_step, hidden_dims]\n",
        "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
        "        # h = lstm_out\n",
        "        # [batch_size, num_layers * num_directions, n_hidden]\n",
        "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
        "        # [batch_size, 1, n_hidden]\n",
        "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
        "        # atten_w [batch_size, 1, hidden_dims]\n",
        "        atten_w = self.attention_layer(lstm_hidden)\n",
        "        # m [batch_size, time_step, hidden_dims]\n",
        "        m = nn.Tanh()(h)\n",
        "        # atten_context [batch_size, 1, time_step]\n",
        "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
        "        # softmax_w [batch_size, 1, time_step]\n",
        "        softmax_w = F.softmax(atten_context, dim=-1)\n",
        "        # context [batch_size, 1, hidden_dims]\n",
        "        context = torch.bmm(softmax_w, h)\n",
        "        result = context.squeeze(1)\n",
        "        return result\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # x : [len_seq, batch_size, embedding_dim]\n",
        "        x = x.permute(1, 0, 2)\n",
        "        output, (final_hidden_state, final_cell_state) = self.lstm_net(x)\n",
        "        # output : [batch_size, len_seq, n_hidden * 2]\n",
        "        output = output.permute(1, 0, 2)\n",
        "        # final_hidden_state : [batch_size, num_layers * num_directions, n_hidden]\n",
        "        final_hidden_state = final_hidden_state.permute(1, 0, 2)\n",
        "        # final_hidden_state = torch.mean(final_hidden_state, dim=0, keepdim=True)\n",
        "        # atten_out = self.attention_net(output, final_hidden_state)\n",
        "        atten_out = self.attention_net_with_w(output, final_hidden_state)\n",
        "        return self.fc_out(atten_out)\n",
        "\n",
        "class AudioBiLSTM(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(AudioBiLSTM, self).__init__()\n",
        "        self.num_classes = config['num_classes']\n",
        "        self.learning_rate = config['learning_rate']\n",
        "        self.dropout = config['dropout']\n",
        "        self.hidden_dims = config['hidden_dims']\n",
        "        self.rnn_layers = config['rnn_layers']\n",
        "        self.embedding_size = config['embedding_size']\n",
        "        self.bidirectional = config['bidirectional']\n",
        "\n",
        "        self.build_model()\n",
        "        # self.init_weight()\n",
        "\n",
        "    def init_weight(net):\n",
        "        for name, param in net.named_parameters():\n",
        "            if not 'ln' in name:\n",
        "                if 'bias' in name:\n",
        "                    nn.init.constant_(param, 0.0)\n",
        "                elif 'weight' in name:\n",
        "                    nn.init.xavier_uniform_(param)\n",
        "\n",
        "    def build_model(self):\n",
        "        # attention layer\n",
        "        self.attention_layer = nn.Sequential(\n",
        "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
        "            nn.ReLU(inplace=True))\n",
        "        # self.attention_weights = self.attention_weights.view(self.hidden_dims, 1)\n",
        "\n",
        "        # self.lstm_net_audio = nn.LSTM(self.embedding_size,\n",
        "        #                         self.hidden_dims,\n",
        "        #                         num_layers=self.rnn_layers,\n",
        "        #                         dropout=self.dropout,\n",
        "        #                         bidirectional=self.bidirectional,\n",
        "        #                         batch_first=True)\n",
        "        self.lstm_net_audio = nn.GRU(self.embedding_size, self.hidden_dims,\n",
        "                                num_layers=self.rnn_layers, dropout=self.dropout, batch_first=True)\n",
        "\n",
        "        self.ln = nn.LayerNorm(self.embedding_size)\n",
        "\n",
        "        # FC层\n",
        "        self.fc_audio = nn.Sequential(\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.hidden_dims, self.num_classes),\n",
        "            # nn.ReLU(),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
        "        '''\n",
        "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
        "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
        "        :return: [batch_size, n_hidden]\n",
        "        '''\n",
        "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
        "        # h [batch_size, time_step, hidden_dims]\n",
        "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
        "        #         h = lstm_out\n",
        "        # [batch_size, num_layers * num_directions, n_hidden]\n",
        "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
        "        # [batch_size, 1, n_hidden]\n",
        "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
        "        # atten_w [batch_size, 1, hidden_dims]\n",
        "        atten_w = self.attention_layer(lstm_hidden)\n",
        "        # m [batch_size, time_step, hidden_dims]\n",
        "        m = nn.Tanh()(h)\n",
        "        # atten_context [batch_size, 1, time_step]\n",
        "       # print(atten_w.shape, m.transpose(1, 2).shape)\n",
        "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
        "        # softmax_w [batch_size, 1, time_step]\n",
        "        softmax_w = F.softmax(atten_context, dim=-1)\n",
        "        # context [batch_size, 1, hidden_dims]\n",
        "        context = torch.bmm(softmax_w, h)\n",
        "        result = context.squeeze(1)\n",
        "        return result\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ln(x)\n",
        "        x, _ = self.lstm_net_audio(x)\n",
        "        x = x.mean(dim=1)\n",
        "        out = self.fc_audio(x)\n",
        "        return out\n",
        "\n",
        "class fusion_net(nn.Module):\n",
        "    def __init__(self, text_embed_size, text_hidden_dims, rnn_layers, dropout, num_classes, \\\n",
        "         audio_hidden_dims, audio_embed_size):\n",
        "        super(fusion_net, self).__init__()\n",
        "        self.text_embed_size = text_embed_size\n",
        "        self.audio_embed_size = audio_embed_size\n",
        "        self.text_hidden_dims = text_hidden_dims\n",
        "        self.audio_hidden_dims = audio_hidden_dims\n",
        "        self.rnn_layers = rnn_layers\n",
        "        self.dropout = dropout\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # ============================= TextBiLSTM =================================\n",
        "\n",
        "        # attention layer\n",
        "        self.attention_layer = nn.Sequential(\n",
        "            nn.Linear(self.text_hidden_dims, self.text_hidden_dims),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # 双层lstm\n",
        "        self.lstm_net = nn.LSTM(self.text_embed_size, self.text_hidden_dims,\n",
        "                                num_layers=self.rnn_layers, dropout=self.dropout,\n",
        "                                bidirectional=True)\n",
        "        # FC层\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.text_hidden_dims, self.text_hidden_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.dropout)\n",
        "        )\n",
        "\n",
        "        # ============================= TextBiLSTM =================================\n",
        "\n",
        "        # ============================= AudioBiLSTM =============================\n",
        "\n",
        "        self.lstm_net_audio = nn.GRU(self.audio_embed_size,\n",
        "                                self.audio_hidden_dims,\n",
        "                                num_layers=self.rnn_layers,\n",
        "                                dropout=self.dropout,\n",
        "                                bidirectional=False,\n",
        "                                batch_first=True)\n",
        "\n",
        "        self.fc_audio = nn.Sequential(\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.audio_hidden_dims, self.audio_hidden_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.dropout)\n",
        "        )\n",
        "\n",
        "        self.ln = nn.LayerNorm(self.audio_embed_size)\n",
        "\n",
        "        # ============================= AudioBiLSTM =============================\n",
        "\n",
        "        # ============================= last fc layer =============================\n",
        "        # self.bn = nn.BatchNorm1d(self.text_hidden_dims + self.audio_hidden_dims)\n",
        "        # modal attention\n",
        "        self.modal_attn = nn.Linear(self.text_hidden_dims + self.audio_hidden_dims, self.text_hidden_dims + self.audio_hidden_dims, bias=False)\n",
        "        self.fc_final = nn.Sequential(\n",
        "            nn.Linear(self.text_hidden_dims + self.audio_hidden_dims, self.num_classes, bias=False),\n",
        "            # nn.ReLU(),\n",
        "            nn.Softmax(dim=1),\n",
        "            # nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
        "        '''\n",
        "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
        "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
        "        :return: [batch_size, n_hidden]\n",
        "        '''\n",
        "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
        "        # h [batch_size, time_step, hidden_dims]\n",
        "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
        "        # [batch_size, num_layers * num_directions, n_hidden]\n",
        "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
        "        # [batch_size, 1, n_hidden]\n",
        "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
        "        # atten_w [batch_size, 1, hidden_dims]\n",
        "        atten_w = self.attention_layer(lstm_hidden)\n",
        "        # m [batch_size, time_step, hidden_dims]\n",
        "        m = nn.Tanh()(h)\n",
        "        # atten_context [batch_size, 1, time_step]\n",
        "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
        "        # softmax_w [batch_size, 1, time_step]\n",
        "        softmax_w = F.softmax(atten_context, dim=-1)\n",
        "        # context [batch_size, 1, hidden_dims]\n",
        "        context = torch.bmm(softmax_w, h)\n",
        "        result = context.squeeze(1)\n",
        "        return result\n",
        "\n",
        "    def pretrained_feature(self, x):\n",
        "        with torch.no_grad():\n",
        "            x_text = []\n",
        "            x_audio = []\n",
        "            for ele in x:\n",
        "                x_text.append(ele[1])\n",
        "                x_audio.append(ele[0])\n",
        "            x_text, x_audio = Variable(torch.tensor(x_text).type(torch.FloatTensor), requires_grad=False), Variable(torch.tensor(x_audio).type(torch.FloatTensor), requires_grad=False)\n",
        "            # ============================= TextBiLSTM =================================\n",
        "            # x : [len_seq, batch_size, embedding_dim]\n",
        "            x_text = x_text.permute(1, 0, 2)\n",
        "            output, (final_hidden_state, _) = self.lstm_net(x_text)\n",
        "            # output : [batch_size, len_seq, n_hidden * 2]\n",
        "            output = output.permute(1, 0, 2)\n",
        "            # final_hidden_state : [batch_size, num_layers * num_directions, n_hidden]\n",
        "            final_hidden_state = final_hidden_state.permute(1, 0, 2)\n",
        "            # final_hidden_state = torch.mean(final_hidden_state, dim=0, keepdim=True)\n",
        "            # atten_out = self.attention_net(output, final_hidden_state)\n",
        "            atten_out = self.attention_net_with_w(output, final_hidden_state)\n",
        "            text_feature = self.fc_out(atten_out)\n",
        "\n",
        "            # ============================= TextBiLSTM =================================\n",
        "\n",
        "            # ============================= AudioBiLSTM =============================\n",
        "            x_audio = self.ln(x_audio)\n",
        "            x_audio, _ = self.lstm_net_audio(x_audio)\n",
        "            x_audio = x_audio.sum(dim=1)\n",
        "            audio_feature = self.fc_audio(x_audio)\n",
        "\n",
        "        # ============================= AudioBiLSTM =============================\n",
        "        return (text_feature, audio_feature)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.bn(x)\n",
        "        # modal_weights = torch.softmax(self.modal_attn(x), dim=1)\n",
        "        # modal_weights = self.modal_attn(x)\n",
        "        # x = (modal_weights * x)\n",
        "        output = self.fc_final(x)\n",
        "        return output\n",
        "\n",
        "class MyLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyLoss, self).__init__()\n",
        "\n",
        "    def forward(self, text_feature, audio_feature, target, model):\n",
        "        weight = model.fc_final[0].weight\n",
        "        # bias = model.fc_final[0].bias\n",
        "        # print(weight, bias)\n",
        "        pred_text = F.linear(text_feature, weight[:, :config['text_hidden_dims']])\n",
        "        pred_audio = F.linear(audio_feature, weight[:, config['text_hidden_dims']:])\n",
        "        l = nn.CrossEntropyLoss()\n",
        "        target = torch.tensor(target)\n",
        "        # l = nn.BCEWithLogitsLoss()\n",
        "        # target = F.one_hot(target, num_classes=2).type(torch.FloatTensor)\n",
        "        # print('y: {}\\npred_audio: {}\\npred_text: {}\\n'.format(target, pred_audio.data.max(1, keepdim=True)[1], pred_text.data.max(1, keepdim=True)[1]))\n",
        "        # return l(pred_text, target) + l(pred_audio, target) + \\\n",
        "        #         config['lambda']*torch.norm(weight[:, :config['text_hidden_dims']]) + \\\n",
        "        #         config['lambda']*torch.norm(weight[:, config['text_hidden_dims']:])\n",
        "        # a = F.softmax(pred_text, dim=1) + F.softmax(pred_audio, dim=1)\n",
        "        return l(pred_text, target) + l(pred_audio, target)"
      ],
      "metadata": {
        "id": "qnhjSKV2iXeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'num_classes': 2,\n",
        "    'dropout': 0.3,\n",
        "    'rnn_layers': 2,\n",
        "    'audio_embed_size': 256,\n",
        "    'text_embed_size': 1024,\n",
        "    'batch_size': 2,\n",
        "    'epochs': 100,\n",
        "    'learning_rate': 8e-6,\n",
        "    'audio_hidden_dims': 256,\n",
        "    'text_hidden_dims': 128,\n",
        "    'cuda': False,\n",
        "    'lambda': 1e-5,\n",
        "}"
      ],
      "metadata": {
        "id": "TeH8buZ8iZNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = fusion_net(config['text_embed_size'], config['text_hidden_dims'], config['rnn_layers'], \\\n",
        "    config['dropout'], config['num_classes'], config['audio_hidden_dims'], config['audio_embed_size'])\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "# optimizer = optim.Adam(model.parameters())\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = MyLoss()"
      ],
      "metadata": {
        "id": "E9k8MT9UiiEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, train_idxs):\n",
        "    global max_train_acc, train_acc\n",
        "    model.train()\n",
        "    batch_idx = 1\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    X_train = []\n",
        "    Y_train = []\n",
        "    for idx in train_idxs:\n",
        "        X_train.append(fuse_features[idx])\n",
        "        Y_train.append(fuse_targets[idx])\n",
        "    for i in range(0, len(X_train), config['batch_size']):\n",
        "        if i + config['batch_size'] > len(X_train):\n",
        "            x, y = X_train[i:], Y_train[i:]\n",
        "        else:\n",
        "            x, y = X_train[i:(i+config['batch_size'])], Y_train[i:(i+config['batch_size'])]\n",
        "        if config['cuda']:\n",
        "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True).cuda(), Variable(torch.from_numpy(y)).cuda()\n",
        "        # 将模型的参数梯度设置为0\n",
        "        optimizer.zero_grad()\n",
        "        text_feature, audio_feature = model.pretrained_feature(x)\n",
        "        # text_feature = torch.from_numpy(ss.fit_transform(text_feature.numpy()))\n",
        "        # audio_feature = torch.from_numpy(ss.fit_transform(audio_feature.numpy()))\n",
        "        # concat_x = torch.cat((audio_feature, text_feature), dim=1)\n",
        "        concat_x = torch.cat((text_feature, audio_feature), dim=1)\n",
        "        # dot_x = text_feature.mul(audio_feature)\n",
        "        # add_x = text_feature.add(audio_feature)\n",
        "        output = model(concat_x)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(torch.tensor(y).data.view_as(pred)).cpu().sum()\n",
        "        # loss = criterion(output, torch.tensor(y))\n",
        "        loss = criterion(text_feature, audio_feature, y, model)\n",
        "        # 后向传播调整参数\n",
        "        loss.backward()\n",
        "        # 根据梯度更新网络参数\n",
        "        optimizer.step()\n",
        "        batch_idx += 1\n",
        "        # loss.item()能够得到张量中的元素值\n",
        "        total_loss += loss.item()\n",
        "    cur_loss = total_loss\n",
        "    max_train_acc = correct\n",
        "    train_acc = correct\n",
        "    print('Train Epoch: {:2d}\\t Learning rate: {:.4f}\\tLoss: {:.6f}\\t Accuracy: {}/{} ({:.0f}%)\\n '.format(\n",
        "                epoch, config['learning_rate'], cur_loss/len(X_train), correct, len(X_train),\n",
        "        100. * correct / len(X_train)))\n",
        "\n",
        "\n",
        "def evaluate(model, test_idxs, fold, train_idxs):\n",
        "    model.eval()\n",
        "    batch_idx = 1\n",
        "    total_loss = 0\n",
        "    pred = torch.empty(config['batch_size'], 1).type(torch.LongTensor)\n",
        "    X_test = []\n",
        "    Y_test = []\n",
        "    for idx in test_idxs:\n",
        "        X_test.append(fuse_features[idx])\n",
        "        Y_test.append(fuse_targets[idx])\n",
        "    global max_train_acc, max_acc,max_f1\n",
        "    for i in range(0, len(X_test), config['batch_size']):\n",
        "        if i + config['batch_size'] > len(X_test):\n",
        "            x, y = X_test[i:], Y_test[i:]\n",
        "        else:\n",
        "            x, y = X_test[i:(i+config['batch_size'])], Y_test[i:(i+config['batch_size'])]\n",
        "        if config['cuda']:\n",
        "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True).cuda(), Variable(torch.from_numpy(y)).cuda()\n",
        "        text_feature, audio_feature = model.pretrained_feature(x)\n",
        "        with torch.no_grad():\n",
        "            # concat_x = torch.cat((audio_feature, text_feature), dim=1)\n",
        "            audio_feature_norm = (audio_feature - audio_feature.mean())/audio_feature.std()\n",
        "            text_feature_norm = (text_feature - text_feature.mean())/text_feature.std()\n",
        "            concat_x = torch.cat((text_feature, audio_feature), dim=1)\n",
        "            output = model(concat_x)\n",
        "        # loss = criterion(output, torch.tensor(y))\n",
        "        loss = criterion(text_feature, audio_feature, y, model)\n",
        "        pred = torch.cat((pred, output.data.max(1, keepdim=True)[1]))\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    y_test_pred, conf_matrix = model_performance(Y_test, pred[config['batch_size']:])\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}'.format(total_loss/len(X_test)))\n",
        "    # custom evaluation metrics\n",
        "    print('Calculating additional test metrics...')\n",
        "    accuracy = float(conf_matrix[0][0] + conf_matrix[1][1]) / np.sum(conf_matrix)\n",
        "    precision = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[0][1])\n",
        "    recall = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[1][0])\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    print(\"Accuracy: {}\".format(accuracy))\n",
        "    print(\"Precision: {}\".format(precision))\n",
        "    print(\"Recall: {}\".format(recall))\n",
        "    print(\"F1-Score: {}\\n\".format(f1_score))\n",
        "    print('='*89)\n",
        "\n",
        "    if max_f1 < f1_score and max_train_acc >= len(train_idxs)*0.9 and f1_score > 0.61:\n",
        "        max_f1 = f1_score\n",
        "        max_acc = accuracy\n",
        "        save(model, f'{BASELINE_DIR}/Model/ClassificationWhole/Fuse/fuse_{max_f1:.2f}_{fold}')\n",
        "        print('*'*64)\n",
        "        print('model saved: f1: {}\\tacc: {}'.format(max_f1, max_acc))\n",
        "        print('*'*64)\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "_8836Yr_lWDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idxs_paths = ['train_idxs_0.63_1.npy', 'train_idxs_0.65_2.npy', 'train_idxs_0.60_3.npy']\n",
        "text_model_paths = ['BiLSTM_128_0.64_1.pt', 'BiLSTM_128_0.66_2.pt', 'BiLSTM_128_0.62_3.pt']\n",
        "audio_model_paths = ['BiLSTM_gru_vlad256_256_0.67_1.pt', 'BiLSTM_gru_vlad256_256_0.67_2.pt', 'BiLSTM_gru_vlad256_256_0.63_3.pt']\n",
        "for fold in range(1, 4):\n",
        "    # if fold != 2:\n",
        "    #     continue\n",
        "    train_idxs_tmp = np.load(f'{BASELINE_DIR}/Features/TrainIdx/{idxs_paths[fold-1]}', allow_pickle=True)\n",
        "    test_idxs_tmp = list(set(list(fuse_dep_idxs)+list(fuse_non_idxs)) - set(train_idxs_tmp))\n",
        "    resample_idxs = list(range(6))\n",
        "\n",
        "    train_idxs, test_idxs = [], []\n",
        "    # depression data augmentation\n",
        "    for idx in train_idxs_tmp:\n",
        "        if idx in fuse_dep_idxs:\n",
        "            feat = fuse_features[idx]\n",
        "            audio_perm = itertools.permutations(feat[0], 3)\n",
        "            text_perm = itertools.permutations(feat[1], 3)\n",
        "            count = 0\n",
        "            for fuse_perm in zip(audio_perm, text_perm):\n",
        "                if count in resample_idxs:\n",
        "                    fuse_features.append(fuse_perm)\n",
        "                    fuse_targets = np.hstack((fuse_targets, 1))\n",
        "                    train_idxs.append(len(fuse_features)-1)\n",
        "                count += 1\n",
        "        else:\n",
        "            train_idxs.append(idx)\n",
        "\n",
        "    for idx in test_idxs_tmp:\n",
        "        if idx in fuse_dep_idxs:\n",
        "            feat = fuse_features[idx]\n",
        "            audio_perm = itertools.permutations(feat[0], 3)\n",
        "            text_perm = itertools.permutations(feat[1], 3)\n",
        "            count = 0\n",
        "            resample_idxs = [0,1,4,5]\n",
        "            for fuse_perm in zip(audio_perm, text_perm):\n",
        "                if count in resample_idxs:\n",
        "                    fuse_features.append(fuse_perm)\n",
        "                    fuse_targets = np.hstack((fuse_targets, 1))\n",
        "                    test_idxs.append(len(fuse_features)-1)\n",
        "                count += 1\n",
        "        else:\n",
        "            test_idxs.append(idx)\n",
        "\n",
        "    text_lstm_model = torch.load(f'{BASELINE_DIR}/Model/ClassificationWhole/Text/{text_model_paths[fold-1]}')\n",
        "    audio_lstm_model = torch.load(f'{BASELINE_DIR}/Model/ClassificationWhole/Audio/{audio_model_paths[fold-1]}')\n",
        "    model_state_dict = {}\n",
        "    model_state_dict['lstm_net_audio.weight_ih_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_ih_l0']\n",
        "    model_state_dict['lstm_net_audio.weight_hh_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_hh_l0']\n",
        "    model_state_dict['lstm_net_audio.bias_ih_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_ih_l0']\n",
        "    model_state_dict['lstm_net_audio.bias_hh_l0'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_hh_l0']\n",
        "\n",
        "    model_state_dict['lstm_net_audio.weight_ih_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_ih_l1']\n",
        "    model_state_dict['lstm_net_audio.weight_hh_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.weight_hh_l1']\n",
        "    model_state_dict['lstm_net_audio.bias_ih_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_ih_l1']\n",
        "    model_state_dict['lstm_net_audio.bias_hh_l1'] = audio_lstm_model.state_dict()['lstm_net_audio.bias_hh_l1']\n",
        "\n",
        "    model_state_dict['fc_audio.1.weight'] = audio_lstm_model.state_dict()['fc_audio.1.weight']\n",
        "    model_state_dict['fc_audio.1.bias'] = audio_lstm_model.state_dict()['fc_audio.1.bias']\n",
        "    model_state_dict['fc_audio.4.weight'] = audio_lstm_model.state_dict()['fc_audio.4.weight']\n",
        "    model_state_dict['fc_audio.4.bias'] = audio_lstm_model.state_dict()['fc_audio.4.bias']\n",
        "\n",
        "    model_state_dict['ln.weight'] = audio_lstm_model.state_dict()['ln.weight']\n",
        "    model_state_dict['ln.bias'] = audio_lstm_model.state_dict()['ln.bias']\n",
        "    model.load_state_dict(text_lstm_model.state_dict(), strict=False)\n",
        "    # model.load_state_dict(audio_lstm_model.state_dict(), strict=False)\n",
        "    model.load_state_dict(model_state_dict, strict=False)\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    model.fc_final[0].weight.requires_grad = True\n",
        "    # model.fc_final[0].bias.requires_grad = True\n",
        "    # model.modal_attn.weight.requires_grad = True\n",
        "\n",
        "    max_f1 = -1\n",
        "    max_acc = -1\n",
        "    max_train_acc = -1\n",
        "\n",
        "    for ep in range(1, config['epochs']):\n",
        "        train(ep, train_idxs)\n",
        "        tloss = evaluate(model, test_idxs, fold, train_idxs)"
      ],
      "metadata": {
        "id": "mD62cvjwfYsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `FuseModelChecking.py`\n",
        "testing module for all the stored models and features altogether"
      ],
      "metadata": {
        "id": "vxFQb7s8fWvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from fuse_net_whole import fusion_net, config, model_performance\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import itertools"
      ],
      "metadata": {
        "id": "9ysjDFRtne8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idxs_paths = ['train_idxs_0.63_1.npy', 'train_idxs_0.65_2.npy', 'train_idxs_0.60_3.npy']\n",
        "text_model_paths = ['BiLSTM_128_0.67_1.pt', 'BiLSTM_128_0.66_2.pt', 'BiLSTM_128_0.66_3.pt']\n",
        "audio_model_paths = ['BiLSTM_gru_vlad256_256_0.63_1.pt', 'BiLSTM_gru_vlad256_256_0.65_2.pt', 'BiLSTM_gru_vlad256_256_0.60_3.pt']\n",
        "fuse_model_paths = ['fuse_0.69_1.pt', 'fuse_0.68_2.pt', 'fuse_0.62_3.pt']\n",
        "\n",
        "text_features = np.load(f'{BASELINE_DIR}/Features/TextWhole/whole_samples_clf_avg.npz')['arr_0']\n",
        "text_targets = np.load(f'{BASELINE_DIR}/Features/TextWhole/whole_labels_clf_avg.npz')['arr_0']\n",
        "audio_features = np.squeeze(np.load(f'{BASELINE_DIR}/Features/AudioWhole/whole_samples_clf_256.npz')['arr_0'], axis=2)\n",
        "audio_targets = np.load(f'{BASELINE_DIR}/Features/AudioWhole/whole_labels_clf_256.npz')['arr_0']\n",
        "\n",
        "fuse_features = [[audio_features[i], text_features[i]] for i in range(text_features.shape[0])]\n",
        "fuse_targets = text_targets\n",
        "\n",
        "fuse_dep_idxs = np.where(text_targets == 1)[0]\n",
        "fuse_non_idxs = np.where(text_targets == 0)[0]"
      ],
      "metadata": {
        "id": "I26L2zOpngY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_idxs):\n",
        "    model.eval()\n",
        "    pred = torch.empty(config['batch_size'], 1).type(torch.LongTensor)\n",
        "    X_test = []\n",
        "    Y_test = []\n",
        "    for idx in test_idxs:\n",
        "        X_test.append(fuse_features[idx])\n",
        "        Y_test.append(fuse_targets[idx])\n",
        "    global max_train_acc, max_acc,max_f1\n",
        "    for i in range(0, len(X_test), config['batch_size']):\n",
        "        if i + config['batch_size'] > len(X_test):\n",
        "            x, y = X_test[i:], Y_test[i:]\n",
        "        else:\n",
        "            x, y = X_test[i:(i+config['batch_size'])], Y_test[i:(i+config['batch_size'])]\n",
        "        if config['cuda']:\n",
        "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True).cuda(), Variable(torch.from_numpy(y)).cuda()\n",
        "        text_feature, audio_feature = model.pretrained_feature(x)\n",
        "        with torch.no_grad():\n",
        "            # concat_x = torch.cat((audio_feature, text_feature), dim=1)\n",
        "            audio_feature_norm = (audio_feature - audio_feature.mean())/audio_feature.std()\n",
        "            text_feature_norm = (text_feature - text_feature.mean())/text_feature.std()\n",
        "            concat_x = torch.cat((text_feature, audio_feature), dim=1)\n",
        "            output = model(concat_x)\n",
        "        pred = torch.cat((pred, output.data.max(1, keepdim=True)[1]))\n",
        "\n",
        "    y_test_pred, conf_matrix = model_performance(Y_test, pred[config['batch_size']:])\n",
        "    # custom evaluation metrics\n",
        "    print('Calculating additional test metrics...')\n",
        "    accuracy = float(conf_matrix[0][0] + conf_matrix[1][1]) / np.sum(conf_matrix)\n",
        "    precision = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[0][1])\n",
        "    recall = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[1][0])\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    print(\"Accuracy: {}\".format(accuracy))\n",
        "    print(\"Precision: {}\".format(precision))\n",
        "    print(\"Recall: {}\".format(recall))\n",
        "    print(\"F1-Score: {}\\n\".format(f1_score))\n",
        "    print('='*89)\n",
        "\n",
        "    return precision, recall, f1_score"
      ],
      "metadata": {
        "id": "rTvKzflMoune"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps, rs, fs = [], [], []\n",
        "for fold in range(3):\n",
        "    train_idxs_tmp = np.load(f'{BASELINE_DIR}/Features/TrainIdx/{idxs_paths[fold]}', allow_pickle=True)\n",
        "    test_idxs_tmp = list(set(list(fuse_dep_idxs)+list(fuse_non_idxs)) - set(train_idxs_tmp))\n",
        "    resample_idxs = list(range(6))\n",
        "    train_idxs, test_idxs = [], []\n",
        "    # depression data augmentation\n",
        "    for idx in train_idxs_tmp:\n",
        "        if idx in fuse_dep_idxs:\n",
        "            feat = fuse_features[idx]\n",
        "            audio_perm = itertools.permutations(feat[0], 3)\n",
        "            text_perm = itertools.permutations(feat[1], 3)\n",
        "            count = 0\n",
        "            for fuse_perm in zip(audio_perm, text_perm):\n",
        "                if count in resample_idxs:\n",
        "                    fuse_features.append(fuse_perm)\n",
        "                    fuse_targets = np.hstack((fuse_targets, 1))\n",
        "                    train_idxs.append(len(fuse_features)-1)\n",
        "                count += 1\n",
        "        else:\n",
        "            train_idxs.append(idx)\n",
        "\n",
        "    for idx in test_idxs_tmp:\n",
        "        if idx in fuse_dep_idxs:\n",
        "            feat = fuse_features[idx]\n",
        "            audio_perm = itertools.permutations(feat[0], 3)\n",
        "            text_perm = itertools.permutations(feat[1], 3)\n",
        "            count = 0\n",
        "            resample_idxs = [0,1,4,5]\n",
        "            for fuse_perm in zip(audio_perm, text_perm):\n",
        "                if count in resample_idxs:\n",
        "                    fuse_features.append(fuse_perm)\n",
        "                    fuse_targets = np.hstack((fuse_targets, 1))\n",
        "                    test_idxs.append(len(fuse_features)-1)\n",
        "                count += 1\n",
        "        else:\n",
        "            test_idxs.append(idx)\n",
        "\n",
        "    fuse_model = torch.load(f'{BASELINE_DIR}/Model/ClassificationWhole/Fuse/{fuse_model_paths[fold]}')\n",
        "    p, r, f = evaluate(fuse_model, test_idxs)\n",
        "    ps.append(p)\n",
        "    rs.append(r)\n",
        "    fs.append(f)\n",
        "print('precison: {} \\n recall: {} \\n f1 score: {}'.format(np.mean(ps), np.mean(rs), np.mean(fs)))"
      ],
      "metadata": {
        "id": "_tCMMnXLfZY7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}